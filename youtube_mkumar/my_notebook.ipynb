{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hive-support</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cbfff3be80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "            .master(\"local[4]\")\\\n",
    "            .appName(\"hive-support\")\\\n",
    "            .config(\"spark.sql.warehouse.dir\",\"C:/Users/Avinash Godbole/Desktop/Programm File/pyspark_code/youtube_manish_kumar/spark_warehouse\")\\\n",
    "            .enableHiveSupport()\\\n",
    "            .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,min, max , avg , sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"id int, name string, age int, salary int, location string, department string\"\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\",\"false\")\\\n",
    "        .schema(schema)\\\n",
    "        .load(\"./emp.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+--------+-----------+\n",
      "|  id|   name| age|salary|location| department|\n",
      "+----+-------+----+------+--------+-----------+\n",
      "|   1| manish|  26| 20000|   india|         IT|\n",
      "|   2|  rahul|null| 40000| germany|engineering|\n",
      "|   3|  pawan|  12| 60000|   india|      sales|\n",
      "|   4|roshini|  44|  null|      uk|engineering|\n",
      "|   5|raushan|  35| 70000|   india|      sales|\n",
      "|   6|   None|  29|200000|      uk|         IT|\n",
      "|   7|   adam|  37| 65000|      us|         IT|\n",
      "|   8|  chris|  16| 40000|      us|      sales|\n",
      "|null|   None|null|  null|    None|       None|\n",
      "|   7|   adam|  37| 65000|      us|         IT|\n",
      "+----+-------+----+------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "|        9|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count, min ,max, avg\n",
    "\n",
    "df.select(count(\"id\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+-------+\n",
      "| total|max(salary)|min(salary)|average|\n",
      "+------+-----------+-----------+-------+\n",
      "|560000|     200000|      20000|  70000|\n",
      "+------+-----------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(sum(\"salary\").alias(\"total\"),max(\"salary\"),min(\"salary\"),avg(\"salary\").cast(\"Int\").alias(\"average\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|       None|    1|\n",
      "|      sales|    3|\n",
      "|         IT|    4|\n",
      "|engineering|    2|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").count().alias(\"total\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+------------------+\n",
      "| department|sum(salary)|min(salary)|max(salary)|       avg(salary)|\n",
      "+-----------+-----------+-----------+-----------+------------------+\n",
      "|       None|       null|       null|       null|              null|\n",
      "|      sales|     170000|      40000|      70000|56666.666666666664|\n",
      "|         IT|     350000|      20000|     200000|           87500.0|\n",
      "|engineering|      40000|      40000|      40000|           40000.0|\n",
      "+-----------+-----------+-----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(sum(\"salary\"),min(\"salary\"),max(\"salary\"),avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| department|sum(salary)|\n",
      "+-----------+-----------+\n",
      "|       None|       null|\n",
      "|      sales|     170000|\n",
      "|         IT|     350000|\n",
      "|engineering|      40000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").agg(sum(\"salary\")).show()\n",
    "df.groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df = spark.read.format(\"csv\")\\\n",
    "            .option(\"header\",True)\\\n",
    "            .option(\"inferSchema\",True)\\\n",
    "            .load(\"./emp2.csv\")\n",
    "\n",
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     dept|sum(salary)|\n",
      "+---------+-----------+\n",
      "|marketing|     220000|\n",
      "|    sales|     150000|\n",
      "|       IT|     295000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy(\"dept\").agg(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----------+\n",
      "|     dept|country|sum(salary)|\n",
      "+---------+-------+-----------+\n",
      "|       IT|  india|     115000|\n",
      "|marketing|  india|     125000|\n",
      "|    sales|     us|      60000|\n",
      "|marketing|     us|      95000|\n",
      "|    sales|  india|      90000|\n",
      "|       IT|     us|     180000|\n",
      "+---------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy(\"dept\",\"country\").agg(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.createTempView(\"emp\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            select dept, sum(salary) from emp group by dept\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     dept|sum(salary)|\n",
      "+---------+-----------+\n",
      "|marketing|     220000|\n",
      "|    sales|     150000|\n",
      "|       IT|     295000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- date_of_joining: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- date_of_purchase: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join \n",
    "\n",
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "(2,'vikash','kolkata',\"12-03-2023\"),\n",
    "(3,'nikita','delhi',\"25-06-2023\"),\n",
    "(4,'rahul','ranchi',\"24-03-2023\"),\n",
    "(5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "(6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "(7,'raman','patna',\"30-12-2022\"),\n",
    "(8,'prakash','ranchi',\"24-02-2023\"),\n",
    "(9,'ragini','kolkata',\"03-03-2023\"),\n",
    "(10,'raushan','jaipur',\"05-02-2023\")]\n",
    "\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "\n",
    "\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "(1,27,5,\"03-02-2023\"),\n",
    "(2,5,3,\"01-06-2023\"),\n",
    "(5,22,1,\"22-03-2023\"),\n",
    "(7,22,4,\"03-02-2023\"),\n",
    "(9,5,6,\"03-03-2023\"),\n",
    "(2,1,12,\"15-06-2023\"),\n",
    "(1,56,2,\"25-06-2023\"),\n",
    "(5,12,5,\"15-04-2023\"),\n",
    "(11,12,76,\"12-03-2023\")]\n",
    "\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "\n",
    "\n",
    "product_data = [(1, 'fanta',20),\n",
    "(2, 'dew',22),\n",
    "(5, 'sprite',40),\n",
    "(7, 'redbull',100),\n",
    "(12,'mazza',45),\n",
    "(22,'coke',27),\n",
    "(25,'limca',21),\n",
    "(27,'pepsi',14),\n",
    "(56,'sting',10)]\n",
    "\n",
    "product_schema=['id','name','price']\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data,customer_schema)\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data,sales_schema)\n",
    "\n",
    "product_df = spark.createDataFrame(product_data,product_schema)\n",
    "\n",
    "customer_df.printSchema()\n",
    "sales_df.printSchema()\n",
    "product_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n",
      "|customer_id|customer_name|address|date_of_joining|\n",
      "+-----------+-------------+-------+---------------+\n",
      "|          1|       manish|  patna|     30-05-2022|\n",
      "|          2|       vikash|kolkata|     12-03-2023|\n",
      "|          3|       nikita|  delhi|     25-06-2023|\n",
      "|          4|        rahul| ranchi|     24-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|\n",
      "|          6|     prantosh|kolkata|     18-10-2022|\n",
      "|          7|        raman|  patna|     30-12-2022|\n",
      "|          8|      prakash| ranchi|     24-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|\n",
      "|         10|      raushan| jaipur|     05-02-2023|\n",
      "+-----------+-------------+-------+---------------+\n",
      "\n",
      "+-----------+----------+--------+----------------+\n",
      "|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+----------+--------+----------------+\n",
      "|          1|        22|      10|      01-06-2022|\n",
      "|          1|        27|       5|      03-02-2023|\n",
      "|          2|         5|       3|      01-06-2023|\n",
      "|          5|        22|       1|      22-03-2023|\n",
      "|          7|        22|       4|      03-02-2023|\n",
      "|          9|         5|       6|      03-03-2023|\n",
      "|          2|         1|      12|      15-06-2023|\n",
      "|          1|        56|       2|      25-06-2023|\n",
      "|          5|        12|       5|      15-04-2023|\n",
      "|         11|        12|      76|      12-03-2023|\n",
      "+-----------+----------+--------+----------------+\n",
      "\n",
      "+---+-------+-----+\n",
      "| id|   name|price|\n",
      "+---+-------+-----+\n",
      "|  1|  fanta|   20|\n",
      "|  2|    dew|   22|\n",
      "|  5| sprite|   40|\n",
      "|  7|redbull|  100|\n",
      "| 12|  mazza|   45|\n",
      "| 22|   coke|   27|\n",
      "| 25|  limca|   21|\n",
      "| 27|  pepsi|   14|\n",
      "| 56|  sting|   10|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.show()\n",
    "sales_df.show()\n",
    "product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner\n",
    "customer_df.join(sales_df,customer_df[\"customer_id\"]==sales_df[\"customer_id\"],\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n",
      "|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n",
      "|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# type of join in spark \n",
    "# inner, outer, left join, right join, left semi join, left anti join, cross join\n",
    "\n",
    "# outer \n",
    "customer_df.join(sales_df, customer_df.customer_id==sales_df.customer_id,\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df, customer_df.customer_id==sales_df.customer_id,\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n",
      "|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n",
      "|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n",
      "|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n",
      "|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n",
      "|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# full outer dimentsion (scd 1 type)\n",
    "customer_df.join(sales_df, customer_df.customer_id==sales_df.customer_id,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n",
      "|customer_id|customer_name|address|date_of_joining|\n",
      "+-----------+-------------+-------+---------------+\n",
      "|          7|        raman|  patna|     30-12-2022|\n",
      "|          9|       ragini|kolkata|     03-03-2023|\n",
      "|          5|       mahesh| jaipur|     22-03-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|\n",
      "|          2|       vikash|kolkata|     12-03-2023|\n",
      "+-----------+-------------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left semi join \n",
    "customer_df.join(sales_df, customer_df.customer_id==sales_df.customer_id,\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n",
      "|customer_id|customer_name|address|date_of_joining|\n",
      "+-----------+-------------+-------+---------------+\n",
      "|          6|     prantosh|kolkata|     18-10-2022|\n",
      "|         10|      raushan| jaipur|     05-02-2023|\n",
      "|          3|       nikita|  delhi|     25-06-2023|\n",
      "|          8|      prakash| ranchi|     24-02-2023|\n",
      "|          4|        rahul| ranchi|     24-03-2023|\n",
      "+-----------+-------------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left anti join\n",
    "customer_df.join(sales_df, customer_df.customer_id==sales_df.customer_id,\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          1|        22|      10|      01-06-2022|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          1|        27|       5|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          2|         5|       3|      01-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          5|        22|       1|      22-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          5|        22|       1|      22-03-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          7|        22|       4|      03-02-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          9|         5|       6|      03-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          7|        22|       4|      03-02-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          9|         5|       6|      03-03-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          2|         1|      12|      15-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|          5|        12|       5|      15-04-2023|\n",
      "|          1|       manish|  patna|     30-05-2022|         11|        12|      76|      12-03-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          1|        56|       2|      25-06-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|          5|        12|       5|      15-04-2023|\n",
      "|          2|       vikash|kolkata|     12-03-2023|         11|        12|      76|      12-03-2023|\n",
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross join\n",
    "customer_df.crossJoin(sales_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.crossJoin(sales_df).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+-------+-------+\n",
      "| id|   name|salary|     dept|country|sum_sal|\n",
      "+---+-------+------+---------+-------+-------+\n",
      "|  3|raushan| 70000|marketing|  india|55000.0|\n",
      "|  6| nikita| 45000|marketing|     us|55000.0|\n",
      "|  7| ragini| 55000|marketing|  india|55000.0|\n",
      "| 10|  rahul| 50000|marketing|     us|55000.0|\n",
      "|  2| vikash| 60000|    sales|     us|75000.0|\n",
      "|  5| pritam| 90000|    sales|  india|75000.0|\n",
      "|  1| manish| 50000|       IT|  india|73750.0|\n",
      "|  4| mukesh| 80000|       IT|     us|73750.0|\n",
      "|  8| rakesh|100000|       IT|     us|73750.0|\n",
      "|  9| aditya| 65000|       IT|  india|73750.0|\n",
      "+---+-------+------+---------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimization of join\n",
    "# window\n",
    "widnow_spec = Window.partitionBy(\"dept\")\n",
    "\n",
    "emp_df.withColumn(\"sum_sal\",avg(\"salary\").over(widnow_spec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, rank, dense_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+-------+----------+---------+---+\n",
      "| id|   name|salary|     dept|country|row_number|dense_rnk|rnk|\n",
      "+---+-------+------+---------+-------+----------+---------+---+\n",
      "|  6| nikita| 45000|marketing|     us|         1|        1|  1|\n",
      "| 10|  rahul| 50000|marketing|     us|         2|        2|  2|\n",
      "|  7| ragini| 55000|marketing|  india|         3|        3|  3|\n",
      "|  3|raushan| 70000|marketing|  india|         4|        4|  4|\n",
      "|  2| vikash| 60000|    sales|     us|         1|        1|  1|\n",
      "|  5| pritam| 90000|    sales|  india|         2|        2|  2|\n",
      "|  1| manish| 50000|       IT|  india|         1|        1|  1|\n",
      "|  9| aditya| 65000|       IT|  india|         2|        2|  2|\n",
      "|  4| mukesh| 80000|       IT|     us|         3|        3|  3|\n",
      "|  8| rakesh|100000|       IT|     us|         4|        4|  4|\n",
      "+---+-------+------+---------+-------+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# optimization of join\n",
    "# window\n",
    "window_spec = Window.partitionBy(\"dept\").orderBy(\"salary\")\n",
    "\n",
    "emp_df.withColumn(\"row_number\",row_number().over(window_spec))\\\n",
    "    .withColumn(\"dense_rnk\",dense_rank().over(window_spec))\\\n",
    "        .withColumn(\"rnk\",rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
