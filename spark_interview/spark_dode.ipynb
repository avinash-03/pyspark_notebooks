{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"interview\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>interview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dfdc587910>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. creae empty rdd and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "# create empty rdd without schema\n",
    "rdd_wthout_schema = spark.sparkContext.emptyRDD()\n",
    "rdd_wthout_schema.collect()\n",
    "\n",
    "# another way to creae rdd \n",
    "rdd1 = spark.sparkContext.parallelize([])\n",
    "\n",
    "# create empty rdd with schema \n",
    "#rdd_empty_schema = \n",
    "\n",
    "# create empty dataframe with schema \n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('id',IntegerType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"email\",StringType(),True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# create empty dataframe \n",
    "df = rdd_wthout_schema.toDF(schema)\n",
    "df.show()\n",
    "# create empty dataframe without schema \n",
    "from pyspark.sql.types import  StructType\n",
    "emptydf_without_schema = spark.createDataFrame([],StructType([]))\n",
    "emptydf_without_schema.show()\n",
    "\n",
    "## create empty dataframe with schema \n",
    "empty_df_withschema = spark.createDataFrame([],schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. script of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15, 12, 20, 18, 28, 30, 40, 40, 50, 55, 60)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. merging the tuple using scala/python\n",
    "tuple_list = [(10,15),(12,20),(18,28),(30,40),(40,50),(55,60)]\n",
    "\n",
    "add_tuple = ()\n",
    "for i in tuple_list:\n",
    "    add_tuple+=i \n",
    "\n",
    "add_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(add_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "list1 = [1,2,3,4,5]\n",
    "list1 = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|Azar|BE|8|BigData|9273564321|Ramesh|BTech|3|Java|8685936472|Partiban|ME|6|dotNet|8756432542|Magesh|MCA|8|DBA|8765435242|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+--------+-----+---+-------+----------+\n",
      "|    name|Field|exp|   tech|       mob|\n",
      "+--------+-----+---+-------+----------+\n",
      "|    Azar|   BE|  8|BigData|9273564321|\n",
      "|  Ramesh|BTech|  3|   Java|8685936472|\n",
      "|Partiban|   ME|  6| dotNet|8756432542|\n",
      "|  Magesh|  MCA|  8|    DBA|8765435242|\n",
      "+--------+-----+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q. input text file holding single row eith pipe delimited as shown below. how \n",
    "# will you apply break to every 5th occurrence of pip delimiter and display as show below.\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace,explode,split\n",
    "# read file \n",
    "df1 = spark.read.csv(\"pipe.txt\")\n",
    "df1.show(truncate=False)\n",
    "\n",
    "# replace evry 5th | with -\n",
    "df2 = df1.withColumn(\"chk\",regexp_replace(\"_c0\",\"(.*?\\\\|){5}\",\"$0-\"))\n",
    "\n",
    "# explode after spliting to get each rows \n",
    "df3 = df2.withColumn(\"col_expl\",explode(split(\"chk\",\"\\|-\"))).select(\"col_expl\")\n",
    "\n",
    "# schema \n",
    "col = [\"name\",\"Field\",\"exp\",\"tech\",\"mob\"]\n",
    "# split on | and give schema \n",
    "df4 = df3.rdd.map(lambda x: x[0].split(\"|\")).toDF(schema=col)\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n",
      "| name|location|  skill1|\n",
      "+-----+--------+--------+\n",
      "|  raj|  mumbai|  python|\n",
      "|  raj|  mumbai|   scala|\n",
      "|  raj|  mumbai| mangodb|\n",
      "|vijay|  mysure|teradata|\n",
      "|vijay|  mysure|   spark|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, act_type: string, act_ts: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# photon intereveiw question\n",
    "#1. read jon file data_photo \n",
    "# output -> raj|Mumbai|python raj|mubai|scala raj|mumbai|magodb vijay|mysure|spark vijay|mysure|ter\n",
    "df = spark.read.format('json').load('data_photo.json')\n",
    "from pyspark.sql.functions import  explode\n",
    "df.withColumn(\"skill1\",explode(\"skill\")).select(\"name\",\"location\",\"skill1\").show()\n",
    "# ========================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. read log file and get error log in separtate file error_runlog.txt using python \n",
    "# pysaprk\n",
    "text = spark.read.text(\"log.txt\")\n",
    "error = text.filter(text.value.contains(\"Error\"))\n",
    "error.write.mode(\"overwrite\").text(\"error_runlog.txt\")\n",
    "\n",
    "# python \n",
    "\n",
    "\n",
    "with open(\"log.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "  \n",
    "with open(\"errorlog.txt\",\"w\") as e:\n",
    "    for row in lines:\n",
    "        if \"Error\" in row:\n",
    "            e.write(row)    \n",
    "\n",
    "# =======================================================================================\n",
    "\n",
    "# 3. find the second letest login tiem for every user\n",
    "login = spark.read.csv(\"login.csv\",inferSchema=True,header=True)\n",
    "# filter login data\n",
    "log_filter = login.filter(login.act_type==\"Login\")\n",
    "# convert to timestamp \n",
    "log_time = log_f.withColumn(\"time\",to_timestamp(unix_timestamp(log_f.act_ts,\"yyyy-MM-dd hh:mm a\")))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from  pyspark.sql.functions import rank,col \n",
    "# partition and order by \n",
    "windowPartition = Window.partitionBy(\"user_id\").orderBy(col(\"time\").desc())\n",
    "final = log_time.withColumn(\"rank\",rank().over(windowPartition))\n",
    "\n",
    "# final output \n",
    "fi = final.filter(final.rank==2).select(\"user_id\",\"act_type\",\"act_ts\")\n",
    "fi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, act_type: string, act_ts: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "login = spark.read.csv(\"login.csv\",inferSchema=True,header=True)\n",
    "login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+\n",
      "|user_id|act_type|             act_ts|\n",
      "+-------+--------+-------------------+\n",
      "|    101|   Login| 2022-05-23 9:00 AM|\n",
      "|    101|  Logout| 2022-05-23 9:30 AM|\n",
      "|    101|   Login|2022-05-23 10:00 AM|\n",
      "|    101|  Logout|2022-05-23 10:30 AM|\n",
      "|    102|   Login| 2022-05-23 9:00 AM|\n",
      "|    102|  Logout| 2022-05-23 9:30 AM|\n",
      "|    102|   Login|2022-05-23 10:00 AM|\n",
      "|    102|  Logout|2022-05-23 10:30 AM|\n",
      "+-------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "login.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import to_timestamp\n",
    "# df=spark.read.csv(fp,header=True)\n",
    "# df=df.withColumn('time',to_timestamp(\"Date\",\"MM/dd/yyyy hh:mm:ss a\"))\n",
    "# df.select(\"Case Number\",'time','Date').show(5,False)\n",
    "\n",
    "#spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "#login.withColumn(\"dat\",to_timestamp(\"act_ts\",\"yyyy-MM-dd hh:mm a\")).show()\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "log_f = login.filter(login.act_type==\"Login\")\n",
    "\n",
    "\n",
    "\n",
    "log_time = log_f.withColumn(\"time\",unix_timestamp(log_f.act_ts,\"yyyy-MM-dd hh:mm a\"))\n",
    "log_time.sort(\"time\").show()\n",
    "from pyspark.sql.window import Window\n",
    "from  pyspark.sql.functions import rank,col \n",
    "windowPartition = Window.partitionBy(\"user_id\").orderBy(col(\"time\").desc())\n",
    "final = log_time.withColumn(\"rank\",rank().over(windowPartition))\n",
    "fi = final.filter(final.rank==2).select(\"user_id\",\"act_type\",\"act_ts\")\n",
    "fi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, to_date, date_format\n",
    "\n",
    "df2 = df1.withColumn(\"time\",to_timestamp(\"input_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|     input_timestamp|               tiem|\n",
      "+---+--------------------+-------------------+\n",
      "|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|to_date(`input_timestamp`, 'MM-dd-yyyy HH:mm:ss.SSSS')|\n",
      "+------------------------------------------------------+\n",
      "|                                                  null|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(to_date(df1.input_timestamp,'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|     input_timestamp|               tiem|\n",
      "+---+--------------------+-------------------+\n",
      "|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Using Cast to convert Timestamp String to DateType\n",
    "df.withColumn('date_type', col('input_timestamp').cast('date')) \\\n",
    "       .show(truncate=False)\n",
    "\n",
    "# Using Cast to convert TimestampType to DateType\n",
    "df.withColumn('date_type', to_timestamp('input_timestamp').cast('date')) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df.select(to_date(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "  \n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"date_type\",to_date(\"input_timestamp\")) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#Timestamp Type to DateType\n",
    "df.withColumn(\"date_type\",to_date(current_timestamp())) \\\n",
    "  .show(truncate=False) \n",
    "\n",
    "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\"))) \\\n",
    "  .withColumn(\"datetype\",to_date(col(\"ts\"))) \\\n",
    "  .show(truncate=False)\n",
    "    \n",
    "#SQL TimestampType to DateType\n",
    "spark.sql(\"select to_date(current_timestamp) as date_type\")\n",
    "#SQL CAST TimestampType to DateType\n",
    "spark.sql(\"select date(to_timestamp('2019-06-24 12:01:19.000')) as date_type\")\n",
    "#SQL CAST timestamp string to DateType\n",
    "spark.sql(\"select date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Timestamp String (default format) to DateType\n",
    "spark.sql(\"select to_date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Custom Timeformat to DateType\n",
    "spark.sql(\"select to_date('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as date_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19\"),(\"2\",\"2019-09-21 10:38:00\"),(\"3\",\"2019-05-21 10:20:00\")],\n",
    "        schema=[\"id\",\"in_time\"])\n",
    "\n",
    "# (\"1\",\"2019-06-24 12:01:19 AM\"),(\"2\",\"2019-09-21 10:38:00 PM)\"\n",
    "\n",
    "df.printSchema()\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "from pyspark.sql.functions import  to_date,to_timestamp,col,unix_timestamp\n",
    "df1 =df.withColumn(\"time\",to_timestamp(unix_timestamp(df.in_time,\"yyyy-MM-dd hh:mm:ss\")))\n",
    "df.printSchema()\n",
    "df1.sort(\"time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01 AM\"),(\"2\",\"2019-09-21 10:38 AM\"),(\"3\",\"2019-05-21 10:20 PM\"),(\"4\",\"2019-05-21 10:20 AM\")],\n",
    "        schema=[\"id\",\"in_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|            in_time|               time|\n",
      "+---+-------------------+-------------------+\n",
      "|  1|2019-06-24 12:01 AM|2019-06-24 00:01:00|\n",
      "|  2|2019-09-21 10:38 AM|2019-09-21 10:38:00|\n",
      "|  3|2019-05-21 10:20 PM|2019-05-21 22:20:00|\n",
      "|  4|2019-05-21 10:20 AM|2019-05-21 10:20:00|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"time\",to_timestamp(unix_timestamp(df.in_time,\"yyyy-MM-dd hh:mm a\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ortho\n",
       "1      robotics\n",
       "2         angle\n",
       "3    pyrosphere\n",
       "Name: productname, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.productname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two csv file \n",
    "\n",
    "with open('emp.csv', 'r') as t1, open('emp2.csv', 'r') as t2:\n",
    "    fileone = t1.readlines()\n",
    "    filetwo = t2.readlines()\n",
    "\n",
    "\n",
    "with open('update.csv', 'w') as outFile:\n",
    "    for line in filetwo:\n",
    "        if line not in fileone:\n",
    "            outFile.write(line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
