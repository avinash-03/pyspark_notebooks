{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>interview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1435dfdf610>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"interview\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"interview\")\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[first_name: string, last_name: string, company_name: string, address: string, city: string, country: string, state: string, zip: int, age: int, phone1: string, phone2: string, email: string, web: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.options(header=True,inferSchema=True).format(\"csv\").load(r\"C:\\workspace\\data\\usdata.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. creae empty rdd and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "# create empty rdd without schema\n",
    "rdd_wthout_schema = spark.sparkContext.emptyRDD()\n",
    "rdd_wthout_schema.collect()\n",
    "\n",
    "# another way to creae rdd \n",
    "rdd1 = spark.sparkContext.parallelize([])\n",
    "\n",
    "# create empty rdd with schema \n",
    "#rdd_empty_schema = \n",
    "\n",
    "# create empty dataframe with schema \n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('id',IntegerType(),True),\n",
    "        StructField(\"name\",StringType(),True),\n",
    "        StructField(\"email\",StringType(),True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# create empty dataframe \n",
    "df = rdd_wthout_schema.toDF(schema)\n",
    "df.show()\n",
    "# create empty dataframe without schema \n",
    "from pyspark.sql.types import  StructType\n",
    "emptydf_without_schema = spark.createDataFrame([],StructType([]))\n",
    "emptydf_without_schema.show()\n",
    "\n",
    "## create empty dataframe with schema \n",
    "empty_df_withschema = spark.createDataFrame([],schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. script of question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15, 12, 20, 18, 28, 30, 40, 40, 50, 55, 60)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. merging the tuple using scala/python\n",
    "tuple_list = [(10,15),(12,20),(18,28),(30,40),(40,50),(55,60)]\n",
    "\n",
    "add_tuple = ()\n",
    "for i in tuple_list:\n",
    "    add_tuple+=i \n",
    "\n",
    "add_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 15, 12, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (10,15)\n",
    "b = (12,20)\n",
    "a+b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(add_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "list1 = [1,2,3,4,5]\n",
    "list1 = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0                                                                                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|Azar|BE|8|BigData|9273564321|Ramesh|BTech|3|Java|8685936472|Partiban|ME|6|dotNet|8756432542|Magesh|MCA|8|DBA|8765435242|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+--------+-----+---+-------+----------+\n",
      "|    name|Field|exp|   tech|       mob|\n",
      "+--------+-----+---+-------+----------+\n",
      "|    Azar|   BE|  8|BigData|9273564321|\n",
      "|  Ramesh|BTech|  3|   Java|8685936472|\n",
      "|Partiban|   ME|  6| dotNet|8756432542|\n",
      "|  Magesh|  MCA|  8|    DBA|8765435242|\n",
      "+--------+-----+---+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q. input text file holding single row eith pipe delimited as shown below. how \n",
    "# will you apply break to every 5th occurrence of pip delimiter and display as show below.\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace,explode,split\n",
    "# read file \n",
    "df1 = spark.read.csv(\"pipe.txt\")\n",
    "df1.show(truncate=False)\n",
    "\n",
    "# replace evry 5th | with -\n",
    "df2 = df1.withColumn(\"chk\",regexp_replace(\"_c0\",\"(.*?\\\\|){5}\",\"$0-\"))\n",
    "\n",
    "# explode after spliting to get each rows \n",
    "df3 = df2.withColumn(\"col_expl\",explode(split(\"chk\",\"\\|-\"))).select(\"col_expl\")\n",
    "\n",
    "# schema \n",
    "col = [\"name\",\"Field\",\"exp\",\"tech\",\"mob\"]\n",
    "# split on | and give schema \n",
    "df4 = df3.rdd.map(lambda x: x[0].split(\"|\")).toDF(schema=col)\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n",
      "| name|location|  skill1|\n",
      "+-----+--------+--------+\n",
      "|  raj|  mumbai|  python|\n",
      "|  raj|  mumbai|   scala|\n",
      "|  raj|  mumbai| mangodb|\n",
      "|vijay|  mysure|teradata|\n",
      "|vijay|  mysure|   spark|\n",
      "+-----+--------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, act_type: string, act_ts: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# photon intereveiw question\n",
    "#1. read jon file data_photo \n",
    "# output -> raj|Mumbai|python raj|mubai|scala raj|mumbai|magodb vijay|mysure|spark vijay|mysure|ter\n",
    "df = spark.read.format('json').load('data_photo.json')\n",
    "from pyspark.sql.functions import  explode\n",
    "df.withColumn(\"skill1\",explode(\"skill\")).select(\"name\",\"location\",\"skill1\").show()\n",
    "# ========================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2. read log file and get error log in separtate file error_runlog.txt using python \n",
    "# pysaprk\n",
    "text = spark.read.text(\"log.txt\")\n",
    "error = text.filter(text.value.contains(\"Error\"))\n",
    "error.write.mode(\"overwrite\").text(\"error_runlog.txt\")\n",
    "\n",
    "# python \n",
    "\n",
    "\n",
    "with open(\"log.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "  \n",
    "with open(\"errorlog.txt\",\"w\") as e:\n",
    "    for row in lines:\n",
    "        if \"Error\" in row:\n",
    "            e.write(row)    \n",
    "\n",
    "# =======================================================================================\n",
    "\n",
    "# 3. find the second letest login tiem for every user\n",
    "login = spark.read.csv(\"login.csv\",inferSchema=True,header=True)\n",
    "# filter login data\n",
    "log_filter = login.filter(login.act_type==\"Login\")\n",
    "# convert to timestamp \n",
    "log_time = log_f.withColumn(\"time\",to_timestamp(unix_timestamp(log_f.act_ts,\"yyyy-MM-dd hh:mm a\")))\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from  pyspark.sql.functions import rank,col \n",
    "# partition and order by \n",
    "windowPartition = Window.partitionBy(\"user_id\").orderBy(col(\"time\").desc())\n",
    "final = log_time.withColumn(\"rank\",rank().over(windowPartition))\n",
    "\n",
    "# final output \n",
    "fi = final.filter(final.rank==2).select(\"user_id\",\"act_type\",\"act_ts\")\n",
    "fi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: int, act_type: string, act_ts: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "login = spark.read.csv(\"login.csv\",inferSchema=True,header=True)\n",
    "login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+\n",
      "|user_id|act_type|             act_ts|\n",
      "+-------+--------+-------------------+\n",
      "|    101|   Login| 2022-05-23 9:00 AM|\n",
      "|    101|  Logout| 2022-05-23 9:30 AM|\n",
      "|    101|   Login|2022-05-23 10:00 AM|\n",
      "|    101|  Logout|2022-05-23 10:30 AM|\n",
      "|    102|   Login| 2022-05-23 9:00 AM|\n",
      "|    102|  Logout| 2022-05-23 9:30 AM|\n",
      "|    102|   Login|2022-05-23 10:00 AM|\n",
      "|    102|  Logout|2022-05-23 10:30 AM|\n",
      "+-------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "login.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import to_timestamp\n",
    "# df=spark.read.csv(fp,header=True)\n",
    "# df=df.withColumn('time',to_timestamp(\"Date\",\"MM/dd/yyyy hh:mm:ss a\"))\n",
    "# df.select(\"Case Number\",'time','Date').show(5,False)\n",
    "\n",
    "#spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "#login.withColumn(\"dat\",to_timestamp(\"act_ts\",\"yyyy-MM-dd hh:mm a\")).show()\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "log_f = login.filter(login.act_type==\"Login\")\n",
    "\n",
    "\n",
    "\n",
    "log_time = log_f.withColumn(\"time\",unix_timestamp(log_f.act_ts,\"yyyy-MM-dd hh:mm a\"))\n",
    "log_time.sort(\"time\").show()\n",
    "from pyspark.sql.window import Window\n",
    "from  pyspark.sql.functions import rank,col \n",
    "windowPartition = Window.partitionBy(\"user_id\").orderBy(col(\"time\").desc())\n",
    "final = log_time.withColumn(\"rank\",rank().over(windowPartition))\n",
    "fi = final.filter(final.rank==2).select(\"user_id\",\"act_type\",\"act_ts\")\n",
    "fi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, to_date, date_format\n",
    "\n",
    "df2 = df1.withColumn(\"time\",to_timestamp(\"input_timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|     input_timestamp|               tiem|\n",
      "+---+--------------------+-------------------+\n",
      "|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+\n",
      "|to_date(`input_timestamp`, 'MM-dd-yyyy HH:mm:ss.SSSS')|\n",
      "+------------------------------------------------------+\n",
      "|                                                  null|\n",
      "+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select(to_date(df1.input_timestamp,'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|     input_timestamp|               tiem|\n",
      "+---+--------------------+-------------------+\n",
      "|  1|2019-06-24 12:01:...|2019-06-24 12:01:19|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Using Cast to convert Timestamp String to DateType\n",
    "df.withColumn('date_type', col('input_timestamp').cast('date')) \\\n",
    "       .show(truncate=False)\n",
    "\n",
    "# Using Cast to convert TimestampType to DateType\n",
    "df.withColumn('date_type', to_timestamp('input_timestamp').cast('date')) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "df.select(to_date(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "  \n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"date_type\",to_date(\"input_timestamp\")) \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "#Timestamp Type to DateType\n",
    "df.withColumn(\"date_type\",to_date(current_timestamp())) \\\n",
    "  .show(truncate=False) \n",
    "\n",
    "df.withColumn(\"ts\",to_timestamp(col(\"input_timestamp\"))) \\\n",
    "  .withColumn(\"datetype\",to_date(col(\"ts\"))) \\\n",
    "  .show(truncate=False)\n",
    "    \n",
    "#SQL TimestampType to DateType\n",
    "spark.sql(\"select to_date(current_timestamp) as date_type\")\n",
    "#SQL CAST TimestampType to DateType\n",
    "spark.sql(\"select date(to_timestamp('2019-06-24 12:01:19.000')) as date_type\")\n",
    "#SQL CAST timestamp string to DateType\n",
    "spark.sql(\"select date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Timestamp String (default format) to DateType\n",
    "spark.sql(\"select to_date('2019-06-24 12:01:19.000') as date_type\")\n",
    "#SQL Custom Timeformat to DateType\n",
    "spark.sql(\"select to_date('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as date_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- in_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19\"),(\"2\",\"2019-09-21 10:38:00\"),(\"3\",\"2019-05-21 10:20:00\")],\n",
    "        schema=[\"id\",\"in_time\"])\n",
    "\n",
    "# (\"1\",\"2019-06-24 12:01:19 AM\"),(\"2\",\"2019-09-21 10:38:00 PM)\"\n",
    "\n",
    "df.printSchema()\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "from pyspark.sql.functions import  to_date,to_timestamp,col,unix_timestamp\n",
    "df1 =df.withColumn(\"time\",to_timestamp(unix_timestamp(df.in_time,\"yyyy-MM-dd hh:mm:ss\")))\n",
    "df.printSchema()\n",
    "df1.sort(\"time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01 AM\"),(\"2\",\"2019-09-21 10:38 AM\"),(\"3\",\"2019-05-21 10:20 PM\"),(\"4\",\"2019-05-21 10:20 AM\")],\n",
    "        schema=[\"id\",\"in_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|            in_time|               time|\n",
      "+---+-------------------+-------------------+\n",
      "|  1|2019-06-24 12:01 AM|2019-06-24 00:01:00|\n",
      "|  2|2019-09-21 10:38 AM|2019-09-21 10:38:00|\n",
      "|  3|2019-05-21 10:20 PM|2019-05-21 22:20:00|\n",
      "|  4|2019-05-21 10:20 AM|2019-05-21 10:20:00|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"time\",to_timestamp(unix_timestamp(df.in_time,\"yyyy-MM-dd hh:mm a\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ortho\n",
       "1      robotics\n",
       "2         angle\n",
       "3    pyrosphere\n",
       "Name: productname, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.productname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare two csv file \n",
    "\n",
    "with open('emp.csv', 'r') as t1, open('emp2.csv', 'r') as t2:\n",
    "    fileone = t1.readlines()\n",
    "    filetwo = t2.readlines()\n",
    "\n",
    "\n",
    "with open('update.csv', 'w') as outFile:\n",
    "    for line in filetwo:\n",
    "        if line not in fileone:\n",
    "            outFile.write(line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning_data(df, bin_data_list: list):\n",
    "    \"\"\" This method creates bins for numerical values based on the column and the bins\n",
    "    defined in the parameter.yml file.\n",
    "\n",
    "    Args:\n",
    "        :param df: Data frame which has the columns to be binned.\n",
    "        :param bin_data_list: A list which defined columns and bin ranges.\n",
    "    Returns:\n",
    "        :rtype: df: Output data frame\n",
    "    \"\"\"\n",
    "\n",
    "    for col_data in bin_data_list:\n",
    "        column_name = col_data.upper()\n",
    "\n",
    "        bin_list = bin_data_list[col_data]\n",
    "        bin_list.insert(0, -float(\"inf\"))\n",
    "        bin_list.append(float(\"inf\"))\n",
    "        splits = bin_list\n",
    "\n",
    "        df = df.withColumn(column_name, df[column_name].cast(DoubleType()))\n",
    "\n",
    "        bucketizer = Bucketizer(\n",
    "            splits=splits, inputCol=column_name, outputCol=column_name + \"_bin123\"\n",
    "        )\n",
    "        df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "        d = create_bin_col_names(bin_data_list[col_data])\n",
    "\n",
    "        mapping_expr = f.create_map([f.lit(x) for x in chain(*d.items())])\n",
    "        df = df.withColumn(\n",
    "            column_name, mapping_expr[f.col(column_name + \"_bin123\")]\n",
    "        ).drop(f.col(column_name + \"_bin123\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_bin_col_names(bin_list: list) -> dict:\n",
    "    \"\"\" This function creates names for bins.\n",
    "\n",
    "        Args:\n",
    "            :param bin_list: A list which contains bins\n",
    "        Returns:\n",
    "            :rtype: dict: Output data dictionary with bin names\n",
    "    \"\"\"\n",
    "\n",
    "    d = {}\n",
    "    x = 0\n",
    "    for item in bin_list:\n",
    "        x += 1\n",
    "        if x == 1:\n",
    "            d.update({x -1 : \"<\" + str(item).strip()})\n",
    "        else:\n",
    "            d.update({x -1 : str(prev_name).strip() + \"-\" + str(item).strip()})\n",
    "        prev_name = item\n",
    "    d.update({x: \">\" + str(item).strip()})\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(company_name,StringType,true),StructField(address,StringType,true),StructField(city,StringType,true),StructField(country,StringType,true),StructField(state,StringType,true),StructField(zip,IntegerType,true),StructField(age,IntegerType,true),StructField(phone1,StringType,true),StructField(phone2,StringType,true),StructField(email,StringType,true),StructField(web,StringType,true)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<age', 1: 'age-zip', 2: '>zip'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_list = [\"age\",\"zip\",]\n",
    "create_bin_col_names(bin_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. find out median \n",
    "output \n",
    "\n",
    "| department | lis_median |\n",
    "|       ---  | ---      |\n",
    "| Sales     | 3.0       |\n",
    "| Finance   | 6.0       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, department: string, state: string, lis: array<bigint>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "simpleData = [(\"202305\",\"Sales\",\"NY\",[1,2,3]),\n",
    "    (\"202306\",\"Sales\",\"NY\",[4,2,3]),\n",
    "    (\"202305\",\"Sales\",\"CA\",[4,5,3]),\n",
    "    (\"202306\",\"Finance\",\"CA\",[4,5,6]),\n",
    "    (\"202305\",\"Finance\",\"NY\",[5,6,7]),\n",
    "    (\"202306\",\"Finance\",\"NY\",[6,7,8]),\n",
    "  ]\n",
    "\n",
    "schema = [\"month\",\"department\",\"state\",\"lis\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, department: string, state: string, lis: array<bigint>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- lis: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'median' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avinash Godbole\\Desktop\\Programm File\\pyspark_code\\spark_interview\\spark_dode.ipynb Cell 40\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark_code/spark_interview/spark_dode.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39mprintSchema()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark_code/spark_interview/spark_dode.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m display(df\u001b[39m.\u001b[39mselectExpr(\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mposexplode(lis) as (pos, lis_value)\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgroupBy(\u001b[39m'\u001b[39m\u001b[39mdepartment\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39magg(median(\u001b[39m\"\u001b[39m\u001b[39mlis_value\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m'\u001b[39m\u001b[39mlis_median\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'median' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()\n",
    "\n",
    "display(df.selectExpr(\"*\", \"posexplode(lis) as (pos, lis_value)\").groupBy('department').agg(median(\"lis_value\").alias('lis_median')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|department|lis_med                          |\n",
      "+----------+---------------------------------+\n",
      "|Sales     |[[1, 2, 3], [4, 2, 3], [4, 5, 3]]|\n",
      "|Finance   |[[4, 5, 6], [5, 6, 7], [6, 7, 8]]|\n",
      "+----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, collect_list\n",
    "from pyspark.sql.types import FloatType\n",
    "def find_median(values_list):\n",
    "    try:\n",
    "        median = np.median(values_list)\n",
    "        return round(float(median),2)\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "med_find=udf(find_median,FloatType())\n",
    "\n",
    "df2 = df.groupBy(\"department\").agg(collect_list(\"lis\").alias(\"lis_med\"))\n",
    "df2.show(truncate=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|department|             lis_med|lis_median|\n",
      "+----------+--------------------+----------+\n",
      "|     Sales|[[1, 2, 3], [4, 2...|       3.0|\n",
      "|   Finance|[[4, 5, 6], [5, 6...|       6.0|\n",
      "+----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"lis_median\",med_find(\"lis_med\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|department|lis_median|\n",
      "+----------+----------+\n",
      "|Sales     |3.0       |\n",
      "|Finance   |6.0       |\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.groupBy(\"department\").agg(flatten(collect_set(col(\"lis\"))).alias(\"cs\")).\\\n",
    "  withColumn('col', array_sort(expr('filter(cs, x -> x is not null)'))).\\\n",
    "  withColumn('lis_median', when(size(col('cs')) % 2 == 0,\n",
    "                                            (expr('col[int(size(cs)/2)]') + expr('col[int(size(cs)/2)-1]'))/2\n",
    "                                            ).otherwise(expr('col[int(size(cs)/2)]'))).\\\n",
    "                                              drop(*['col','cs']).\\\n",
    "  show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [[1, 2, 3], [4, 2, 3], [4, 5, 3]]\n",
    "np.median(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set2 = [142, 140, 130, 150, 160, 135, 158,132]\n",
    "int(np.median(set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: '2', 3: '3', 4: '4'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def three_sum(arr : list,bin_data : dict) -> list:\n",
    "    for item in arr:\n",
    "        if item not in bin_data:\n",
    "            bin_data[item] = str(item) \n",
    "        else:\n",
    "            bin_data[item] += str(item) \n",
    "    return bin_data  \n",
    "\n",
    "\n",
    "arr = [1,2,3,4,5,8,4,7,0,10,3,5]\n",
    "\n",
    "three_sum({2,3,4,4},{})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>interview</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21338d34c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|node|parent|\n",
      "+----+------+\n",
      "|   1|     5|\n",
      "|   2|     5|\n",
      "|   3|     6|\n",
      "|   4|     6|\n",
      "|   5|     7|\n",
      "|   6|     7|\n",
      "|   7|  null|\n",
      "+----+------+\n",
      "\n",
      "+----+------+----------+\n",
      "|node|parent|    result|\n",
      "+----+------+----------+\n",
      "|   1|     5| leaf_node|\n",
      "|   2|     5| leaf_node|\n",
      "|   3|     6| leaf_node|\n",
      "|   4|     6| leaf_node|\n",
      "|   5|     7|inner_node|\n",
      "|   6|     7|inner_node|\n",
      "|   7|  null| root_node|\n",
      "+----+------+----------+\n",
      "\n",
      "+----+------+---------+\n",
      "|node|parent|   result|\n",
      "+----+------+---------+\n",
      "|   7|  null|root_node|\n",
      "|   6|     7|innernode|\n",
      "|   5|     7|innernode|\n",
      "|   1|     5|leaf_node|\n",
      "|   3|     6|leaf_node|\n",
      "|   2|     5|leaf_node|\n",
      "|   4|     6|leaf_node|\n",
      "+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new column result with name root , leaf and inner node \n",
    "from pyspark.sql import functions as F\n",
    "node = [1,2,3,4,5,6,7]\n",
    "parent = [5,5,6,6,7,7,None]\n",
    "\n",
    "data = list(zip(node,parent))\n",
    "data\n",
    "\n",
    "df = spark.createDataFrame(data,[\"node\",\"parent\"])\n",
    "df.show()\n",
    "\n",
    "# using collect\n",
    "parent_list = df.rdd.map(lambda x: x[1]).collect()\n",
    "df.withColumn(\"result\",\n",
    "              F.when(df.parent.isNull(),\"root_node\")\n",
    "              .when(df.node.isin(parent_list),\"inner_node\")\n",
    "              .otherwise(\"leaf_node\")\n",
    "              ).show()\n",
    "\n",
    "\n",
    "# using join\n",
    "df.join(df.alias(\"df1\"),df.node==F.col(\"df1.parent\"),\"left\")\\\n",
    "        .withColumn(\"result\",F.when(df.parent.isNull(),\"root_node\")\\\n",
    "        .when(F.col(\"df1.node\").isNull(),\"leaf_node\")\\\n",
    "        .otherwise(\"innernode\"))\\\n",
    "        .select(df.node,df.parent,\"result\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- node: long (nullable = true)\n",
      " |-- parent: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "node = [1,2,3,4,5,6,7]\n",
    "parent = [5,5,6,6,7,7,None]\n",
    "\n",
    "data = list(zip(node,parent))\n",
    "data\n",
    "df = spark.createDataFrame(data,[\"node\",\"parent\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list,collect_set,array_contains,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "grouping expressions sequence is empty, and '`node`' is not an aggregate function. Wrap '(collect_set(`parent`) AS `new`)' in windowing function(s) or wrap '`node`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [node#0L, parent#1L, collect_set(parent#1L, 0, 0) AS new#106]\n+- LogicalRDD [node#0L, parent#1L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avinash Godbole\\Desktop\\Programm File\\pyspark_code\\spark_interview\\spark_dode.ipynb Cell 53\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark_code/spark_interview/spark_dode.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mwithColumn(\u001b[39m\"\u001b[39;49m\u001b[39mnew\u001b[39;49m\u001b[39m\"\u001b[39;49m,lit(collect_set(\u001b[39m\"\u001b[39;49m\u001b[39mparent\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\dataframe.py:2096\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   2076\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2077\u001b[0m \u001b[39mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[0;32m   2078\u001b[0m \u001b[39mexisting column that has the same name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2093\u001b[0m \n\u001b[0;32m   2094\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2095\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column), \u001b[39m\"\u001b[39m\u001b[39mcol should be Column\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 2096\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    130\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    132\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     raise_from(converted)\n\u001b[0;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: grouping expressions sequence is empty, and '`node`' is not an aggregate function. Wrap '(collect_set(`parent`) AS `new`)' in windowing function(s) or wrap '`node`' in first() (or first_value) if you don't care which value you get.;;\nAggregate [node#0L, parent#1L, collect_set(parent#1L, 0, 0) AS new#106]\n+- LogicalRDD [node#0L, parent#1L], false\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new\",lit(collect_set(\"parent\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '(`node` IN (collect_list(`parent`)))' due to data type mismatch: Arguments must be same type but were: bigint != array<bigint>;;\n'Filter node#0L IN (collect_list(parent#1L, 0, 0))\n+- LogicalRDD [node#0L, parent#1L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avinash Godbole\\Desktop\\Programm File\\pyspark_code\\spark_interview\\spark_dode.ipynb Cell 54\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark_code/spark_interview/spark_dode.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mfilter(df\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49misin(collect_list(\u001b[39m\"\u001b[39;49m\u001b[39mparent\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\dataframe.py:1461\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[1;34m(self, condition)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mfilter(condition)\n\u001b[0;32m   1460\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(condition, Column):\n\u001b[1;32m-> 1461\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mfilter(condition\u001b[39m.\u001b[39;49m_jc)\n\u001b[0;32m   1462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcondition should be string or Column\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    130\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    132\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     raise_from(converted)\n\u001b[0;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve '(`node` IN (collect_list(`parent`)))' due to data type mismatch: Arguments must be same type but were: bigint != array<bigint>;;\n'Filter node#0L IN (collect_list(parent#1L, 0, 0))\n+- LogicalRDD [node#0L, parent#1L], false\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.node.isin(collect_list(\"parent\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o76.parquet.\n: java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException\r\n\tat org.apache.hadoop.fs.s3.S3FileSystem.createDefaultStore(S3FileSystem.java:99)\r\n\tat org.apache.hadoop.fs.s3.S3FileSystem.initialize(S3FileSystem.java:89)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:758)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: org.jets3t.service.S3ServiceException\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avinash Godbole\\Desktop\\Programm File\\pyspark_code\\spark_interview\\spark_dode.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark_code/spark_interview/spark_dode.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_shoes \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mparquet(\u001b[39m\"\u001b[39;49m\u001b[39ms3://amazon-reviews-pds/parquet/product_category=Shoes/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark_code/spark_interview/spark_dode.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_shoes\u001b[39m.\u001b[39mprintSchema()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\readwriter.py:353\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    350\u001b[0m recursiveFileLookup \u001b[39m=\u001b[39m options\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mrecursiveFileLookup\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(mergeSchema\u001b[39m=\u001b[39mmergeSchema, pathGlobFilter\u001b[39m=\u001b[39mpathGlobFilter,\n\u001b[0;32m    352\u001b[0m                recursiveFileLookup\u001b[39m=\u001b[39mrecursiveFileLookup)\n\u001b[1;32m--> 353\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mparquet(_to_seq(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc, paths)))\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    130\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.parquet.\n: java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException\r\n\tat org.apache.hadoop.fs.s3.S3FileSystem.createDefaultStore(S3FileSystem.java:99)\r\n\tat org.apache.hadoop.fs.s3.S3FileSystem.initialize(S3FileSystem.java:89)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\r\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\r\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:758)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.ClassNotFoundException: org.jets3t.service.S3ServiceException\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "df_shoes = spark.read.parquet(\"s3://amazon-reviews-pds/parquet/product_category=Shoes/\")\n",
    "df_shoes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     M|\n",
      "|     M|\n",
      "|     F|\n",
      "|     M|\n",
      "|     F|\n",
      "|     F|\n",
      "|     F|\n",
      "+------+\n",
      "\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|    4|\n",
      "|     M|    3|\n",
      "+------+-----+\n",
      "\n",
      "+------+----+----+\n",
      "|gender|   F|   M|\n",
      "+------+----+----+\n",
      "|     F|   4|null|\n",
      "|     M|null|   3|\n",
      "+------+----+----+\n",
      "\n",
      "+-----+-----+\n",
      "|M_cnt|F_cnt|\n",
      "+-----+-----+\n",
      "|    3|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender = [\"M\",\"M\",\"F\",\"M\",\"F\",\"F\",\"F\"]\n",
    "gender = [(i,) for i in gender]\n",
    "\n",
    "col = ['gender']\n",
    "df = spark.createDataFrame(gender,col)\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "df.show()\n",
    "df.groupBy(\"gender\").count().show()\n",
    "df.groupBy(\"gender\").pivot(\"gender\").count().show()\n",
    "\n",
    "df.createTempView(\"tbl_gender\")\n",
    "\n",
    "query = \"select sum(case when gender='M' then 1 else 0 end) as M_cnt , sum(case when gender='F' then 1 else 0 end) as F_cnt from tbl_gender\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|M_cnt|F_cnt|\n",
      "+-----+-----+\n",
      "|    3|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"sum(case when gender='M' then 1 else 0 end) as M_cnt\"\n",
    "query2 = \"sum(case when gender='F' then 1 else 0 end) as F_cnt\" \n",
    "df.selectExpr(query1,query2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
