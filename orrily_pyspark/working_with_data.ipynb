{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('workingWithData').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>workingWithData</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24bc5d2c190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 2 Working with data\n",
    "# chapter 5 Partitioning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(iterator):\n",
    "    print(\"elements=\", list(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partition \n",
    "numbers = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(numbers)\n",
    "num_partition = rdd.getNumPartitions()\n",
    "num_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22/09/05 13:42:02 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, avinash, executor driver, partition 2, PROCESS_LOCAL, 7360 bytes)\n",
    "22/09/05 13:42:02 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, avinash, executor driver, partition 3, PROCESS_LOCAL, 7387 bytes)\n",
    "22/09/05 13:42:03 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, avinash, executor driver, partition 4, PROCESS_LOCAL, 7360 bytes)\n",
    "22/09/05 13:42:03 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, avinash, executor driver, partition 5, PROCESS_LOCAL, 7387 bytes)\n",
    "22/09/05 13:42:03 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, avinash, executor driver, partition 6, PROCESS_LOCAL, 7360 bytes)\n",
    "22/09/05 13:42:03 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, avinash, executor driver, partition 7, PROCESS_LOCAL, 7387 bytes)\n",
    "22/09/05 13:42:03 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
    "22/09/05 13:42:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)      \n",
    "22/09/05 13:42:03 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)      \n",
    "22/09/05 13:42:03 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)      \n",
    "22/09/05 13:42:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)      \n",
    "22/09/05 13:42:03 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)      \n",
    "22/09/05 13:42:03 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)      \n",
    "22/09/05 13:42:03 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
    "elements= [10]\n",
    "22/09/05 13:42:04 INFO PythonRunner: Times: total = 737, boot = 722, init = 15, finish = 0\n",
    "elements= [1]\n",
    "22/09/05 13:42:04 INFO PythonRunner: Times: total = 1348, boot = 1348, init = 0, finish = 0\n",
    "elements= [5, 6]\n",
    "22/09/05 13:42:05 INFO PythonRunner: Times: total = 1992, boot = 1992, init = 0, finish = 0\n",
    "elements= [11, 12]\n",
    "22/09/05 13:42:06 INFO PythonRunner: Times: total = 2619, boot = 2619, init = 0, finish = 0\n",
    "elements= [8, 9]\n",
    "22/09/05 13:42:06 INFO PythonRunner: Times: total = 3248, boot = 3248, init = 0, finish = 0\n",
    "elements= [2, 3]\n",
    "22/09/05 13:42:07 INFO PythonRunner: Times: total = 3879, boot = 3879, init = 0, finish = 0\n",
    "elements= [4]\n",
    "22/09/05 13:42:07 INFO PythonRunner: Times: total = 4503, boot = 4503, init = 0, finish = 0\n",
    "elements= [7]\n",
    "22/09/05 13:42:08 INFO PythonRunner: Times: total = 5145, boot = 5145, init = 0, finish = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = spark.read.option(\"inferschema\",\"true\")\\\n",
    "    .csv('data/customers_with_date.txt').toDF('customer_id','date','trnx_Id','amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+------+\n",
      "|customer_id|     date|trnx_Id|amount|\n",
      "+-----------+---------+-------+------+\n",
      "|         c1| 2/9/2019|  T0011|    20|\n",
      "|         c1| 2/9/2019|  T0012|    12|\n",
      "|         c1| 3/9/2019|  T0013|    30|\n",
      "|         c1| 3/9/2019|  T0014|    42|\n",
      "|         c1|4/12/2019|  T0023|    48|\n",
      "|         c1|4/12/2018|  T0051|    28|\n",
      "|         c1|4/12/2019|  T0043|    42|\n",
      "|         c1|4/12/2018|  T0091|    29|\n",
      "|         c1| 1/3/2018|  T0002|    12|\n",
      "|         c1| 4/3/2018|  T0003|    44|\n",
      "|         c2|2/10/2019|  T0511|    20|\n",
      "|         c2|2/10/2019|  T0612|    17|\n",
      "|         c2| 2/9/2019|  T0061|    25|\n",
      "|         c2| 2/9/2019|  T0062|    78|\n",
      "|         c2|3/12/2019|  T0513|    67|\n",
      "|         c2|3/12/2019|  T0014|    42|\n",
      "|         c2|4/10/2019|  T0023|    48|\n",
      "|         c2|4/10/2018|  T0051|    28|\n",
      "|         c2|4/12/2019|  T0043|    42|\n",
      "|         c2|4/12/2018|  T0091|    29|\n",
      "+-----------+---------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType,StringType\n",
    "\n",
    "#-------------------------------------\n",
    "# date_as_str: day/month/year\n",
    "@udf(returnType=IntegerType())\n",
    "def get_year(date_as_str):\n",
    "    tokens = date_as_str.split(\"/\")\n",
    "    return int(tokens[2])\n",
    "#end-def\n",
    "#-------------------------------------\n",
    "# date_as_str: day/month/year\n",
    "@udf(returnType=IntegerType())\n",
    "def get_month(date_as_str):\n",
    "    tokens = date_as_str.split(\"/\")\n",
    "    return int(tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with year and month\n",
    "df2 = df.withColumn(\"year\",get_year(df.date))\\\n",
    "    .withColumn(\"month\",get_month(df.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+------+----+-----+\n",
      "|customer_id|     date|trnx_Id|amount|year|month|\n",
      "+-----------+---------+-------+------+----+-----+\n",
      "|         c1| 2/9/2019|  T0011|    20|2019|    9|\n",
      "|         c1| 2/9/2019|  T0012|    12|2019|    9|\n",
      "|         c1| 3/9/2019|  T0013|    30|2019|    9|\n",
      "|         c1| 3/9/2019|  T0014|    42|2019|    9|\n",
      "|         c1|4/12/2019|  T0023|    48|2019|   12|\n",
      "|         c1|4/12/2018|  T0051|    28|2018|   12|\n",
      "|         c1|4/12/2019|  T0043|    42|2019|   12|\n",
      "|         c1|4/12/2018|  T0091|    29|2018|   12|\n",
      "|         c1| 1/3/2018|  T0002|    12|2018|    3|\n",
      "|         c1| 4/3/2018|  T0003|    44|2018|    3|\n",
      "|         c2|2/10/2019|  T0511|    20|2019|   10|\n",
      "|         c2|2/10/2019|  T0612|    17|2019|   10|\n",
      "|         c2| 2/9/2019|  T0061|    25|2019|    9|\n",
      "|         c2| 2/9/2019|  T0062|    78|2019|    9|\n",
      "|         c2|3/12/2019|  T0513|    67|2019|   12|\n",
      "|         c2|3/12/2019|  T0014|    42|2019|   12|\n",
      "|         c2|4/10/2019|  T0023|    48|2019|   10|\n",
      "|         c2|4/10/2018|  T0051|    28|2018|   10|\n",
      "|         c2|4/12/2019|  T0043|    42|2019|   12|\n",
      "|         c2|4/12/2018|  T0091|    29|2018|   12|\n",
      "+-----------+---------+-------+------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to partition\n",
    "df2.write.partitionBy(\"year\",'month')\\\n",
    "    .parquet('output/parquetdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,StringType\n",
    "\n",
    "#-------------------------------------\n",
    "# date_as_str: day/month/year\n",
    "@udf(returnType=StringType())\n",
    "def get_year1(date_as_str):\n",
    "    tokens = date_as_str.split(\"/\")\n",
    "    return tokens[2]\n",
    "#end-def\n",
    "#-------------------------------------\n",
    "# date_as_str: day/month/year\n",
    "@udf(returnType=StringType())\n",
    "def get_month1(date_as_str):\n",
    "    tokens = date_as_str.split(\"/\")\n",
    "    return tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column with year and month\n",
    "df3 = df.withColumn(\"year\",get_year1(df.date))\\\n",
    "    .withColumn(\"month\",get_month1(df.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+------+----+-----+\n",
      "|customer_id|     date|trnx_Id|amount|year|month|\n",
      "+-----------+---------+-------+------+----+-----+\n",
      "|         c1| 2/9/2019|  T0011|    20|2019|    9|\n",
      "|         c1| 2/9/2019|  T0012|    12|2019|    9|\n",
      "|         c1| 3/9/2019|  T0013|    30|2019|    9|\n",
      "|         c1| 3/9/2019|  T0014|    42|2019|    9|\n",
      "|         c1|4/12/2019|  T0023|    48|2019|   12|\n",
      "|         c1|4/12/2018|  T0051|    28|2018|   12|\n",
      "|         c1|4/12/2019|  T0043|    42|2019|   12|\n",
      "|         c1|4/12/2018|  T0091|    29|2018|   12|\n",
      "|         c1| 1/3/2018|  T0002|    12|2018|    3|\n",
      "|         c1| 4/3/2018|  T0003|    44|2018|    3|\n",
      "|         c2|2/10/2019|  T0511|    20|2019|   10|\n",
      "|         c2|2/10/2019|  T0612|    17|2019|   10|\n",
      "|         c2| 2/9/2019|  T0061|    25|2019|    9|\n",
      "|         c2| 2/9/2019|  T0062|    78|2019|    9|\n",
      "|         c2|3/12/2019|  T0513|    67|2019|   12|\n",
      "|         c2|3/12/2019|  T0014|    42|2019|   12|\n",
      "|         c2|4/10/2019|  T0023|    48|2019|   10|\n",
      "|         c2|4/10/2018|  T0051|    28|2018|   10|\n",
      "|         c2|4/12/2019|  T0043|    42|2019|   12|\n",
      "|         c2|4/12/2018|  T0091|    29|2018|   12|\n",
      "+-----------+---------+-------+------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- trnx_Id: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Text data source does not support int data type.;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cbc356d9ddf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# write to partition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"year\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'month'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output/textdata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[1;34m(self, path, compression, lineSep)\u001b[0m\n\u001b[0;32m    955\u001b[0m         \"\"\"\n\u001b[0;32m    956\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Text data source does not support int data type.;"
     ]
    }
   ],
   "source": [
    "# write to partition\n",
    "df3.write.partitionBy(\"year\",'month')\\\n",
    "    .text('output/textdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 7. interacting with external Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mysql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load(path=None,format=None,schema=None,**options)\n",
    "# read rdbms table   spark.read.format('jdbc').options(url=url,driver=driver,dbtable=dbtable,user=username,password=password).load()\n",
    "dataframe_mysql = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option('url',\"jdbc:mysql://localhost\")\\\n",
    "    .option(\"driver\",\"com.mysql.jdbc.Driver\")\\\n",
    "    .option('dbtable','metadb.dept')\\\n",
    "    .option('user','root')\\\n",
    "    .option('password','root')\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+-------+\n",
      "|dept_number| dept_name|dept_location|manager|\n",
      "+-----------+----------+-------------+-------+\n",
      "|         10|ACCOUNTING| NEW YORK, NY|   alex|\n",
      "|         20|  RESEARCH|   DALLAS, TX|   alex|\n",
      "|         30|     SALES|  CHICAGO, IL|   jane|\n",
      "|         40|OPERATIONS|   BOSTON, MA|   jane|\n",
      "|         50| MARKETING|Sunnyvale, CA|   jane|\n",
      "|         60|  SOFTWARE| Stanford, CA|   jane|\n",
      "|         70|  HARDWARE|   BOSTON, MA| sophia|\n",
      "+-----------+----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_mysql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_number: integer (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_location: string (nullable = true)\n",
      " |-- manager: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_mysql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|dept_number|manager|\n",
      "+-----------+-------+\n",
      "|         10|   alex|\n",
      "|         20|   alex|\n",
      "|         30|   jane|\n",
      "|         40|   jane|\n",
      "|         50|   jane|\n",
      "|         60|   jane|\n",
      "|         70| sophia|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show dept_number and manageer \n",
    "dataframe_mysql.select(\"dept_number\",'manager').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|manager|count|\n",
      "+-------+-----+\n",
      "|   jane|    4|\n",
      "|   alex|    2|\n",
      "| sophia|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_mysql.select(\"dept_number\",\"manager\").groupBy(\"manager\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+-------+\n",
      "|dept_number| dept_name|dept_location|manager|\n",
      "+-----------+----------+-------------+-------+\n",
      "|         10|ACCOUNTING| NEW YORK, NY|   alex|\n",
      "|         20|  RESEARCH|   DALLAS, TX|   alex|\n",
      "|         30|     SALES|  CHICAGO, IL|   jane|\n",
      "|         40|OPERATIONS|   BOSTON, MA|   jane|\n",
      "|         50| MARKETING|Sunnyvale, CA|   jane|\n",
      "|         60|  SOFTWARE| Stanford, CA|   jane|\n",
      "|         70|  HARDWARE|   BOSTON, MA| sophia|\n",
      "+-----------+----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to run fulley fledged quiery against dataframe first register dataframe as table\n",
    "\n",
    "dataframe_mysql.registerTempTable('mydept')\n",
    "\n",
    "spark.sql(\"select * from mydept\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------------+-------+\n",
      "|dept_number| dept_name|dept_location|manager|\n",
      "+-----------+----------+-------------+-------+\n",
      "|         10|ACCOUNTING| NEW YORK, NY|   alex|\n",
      "|         20|  RESEARCH|   DALLAS, TX|   alex|\n",
      "|         30|     SALES|  CHICAGO, IL|   jane|\n",
      "+-----------+----------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from mydept where dept_number < 40 order by dept_name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|name|age|salary|\n",
      "+----+---+------+\n",
      "|alex| 60| 18000|\n",
      "|adel| 40| 45000|\n",
      "|adel| 50| 77000|\n",
      "|jane| 40| 52000|\n",
      "|jane| 60| 81000|\n",
      "|alex| 50| 62000|\n",
      "|mary| 50| 92000|\n",
      "|mary| 60| 63000|\n",
      "|mary| 40| 55000|\n",
      "|mary| 40| 55000|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triplets = [ (\"alex\", 60, 18000),\n",
    "... (\"adel\", 40, 45000),\n",
    "... (\"adel\", 50, 77000),\n",
    "... (\"jane\", 40, 52000),\n",
    "... (\"jane\", 60, 81000),\n",
    "... (\"alex\", 50, 62000),\n",
    "... (\"mary\", 50, 92000),\n",
    "... (\"mary\", 60, 63000),\n",
    "... (\"mary\", 40, 55000),\n",
    "... (\"mary\", 40, 55000)\n",
    "... ]\n",
    "\n",
    "triplet_df = spark.createDataFrame(triplets,['name','age','salary'])\n",
    "triplet_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to mysql\n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "url = \"jdbc:mysql://localhost\"\n",
    "username= 'root'\n",
    "password = 'root'\n",
    "dbtable='metadb.triplets'\n",
    "\n",
    "\n",
    "triplet_df.write.format('jdbc').options(url=url,driver=driver,dbtable=dbtable,user=username,password=password)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modes = (append,overwrite,ignore,error(default if data aleready existst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatting new dataframe from csv files\n",
    "emp_df = spark.read.csv('data/emp.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| dept|name|hours|\n",
      "+-----+----+-----+\n",
      "|Sales|Barb|   40|\n",
      "|Sales| Dan|   20|\n",
      "|   IT|Alex|   22|\n",
      "|   IT|Jane|   24|\n",
      "|   HR|Alex|   20|\n",
      "|   HR|Mary|   30|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df1 = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .options(header=True,inferschema=True)\\\n",
    "    .load('data/emp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| dept|name|hours|\n",
      "+-----+----+-----+\n",
      "|Sales|Barb|   40|\n",
      "|Sales| Dan|   20|\n",
      "|   IT|Alex|   22|\n",
      "|   IT|Jane|   24|\n",
      "|   HR|Alex|   20|\n",
      "|   HR|Mary|   30|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- hours: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.write.csv('employee.csv',header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json files\n",
    "data_path = 'data/emp.json'\n",
    "json_df = spark.read.json(data_path)\n",
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.format('json')\\\n",
    "    .load([data_path, data_path])\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   key|value|\n",
      "+------+-----+\n",
      "|  name| alex|\n",
      "|gender| male|\n",
      "| state|   CA|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write json\n",
    "data = [(\"name\", \"alex\"), (\"gender\", \"male\"), (\"state\",\n",
    "\"CA\")]\n",
    "\n",
    "df =spark.createDataFrame(data,['key','value'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.json('tmp/data') # want only one file -> df.repartition(1).write.json('data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing form Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing to aws S3\n",
    "## http://s3.<region>.amazonaws.com/<bucket>/<key>\n",
    "## http://s3.us-east-1.amazonaws.com/project-dev/dna/sample123.vcf\n",
    "\n",
    "# # spark: SparkSession\n",
    "# sc = spark.sparkContext\n",
    "# # set access key\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3.awsAccessKeyId\", \"AKIAI74O5KPLUQGVOJWQ\")\n",
    "# # set secret key\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.s3.awsSecretAccessKey\", \"LmuKE7afdasdfxK2vj1nfA0Bp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_object_path = \"s3n://bucket-name/object-path\"\n",
    "# df = spark.read.text(s3_object_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use boto3\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = 'avinashhivedata'\n",
    "key = 'usdata1.csv'\n",
    "obj = s3.Object(bucket,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = obj.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s3_object_path='s3a://avinashhivedata/usdata1.csv'\n",
    "df = spark.read.text(s3_object_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|first_name,last_n...|\n",
      "|James,Butt,\"Bento...|\n",
      "|Josephine,Darakjy...|\n",
      "|Art,Venere,\"Cheme...|\n",
      "|Lenna,Paprocki,Fe...|\n",
      "|Donette,Foller,Pr...|\n",
      "|Simona,Morasca,\"C...|\n",
      "|Mitsue,Tollner,Mo...|\n",
      "|Leota,Dilliard,Co...|\n",
      "|Sage,Wieser,Truhl...|\n",
      "|Kris,Marrier,\"Kin...|\n",
      "|Minna,Amigon,\"Dor...|\n",
      "|Abel,Maclead,Rang...|\n",
      "|Kiley,Caldarera,F...|\n",
      "|Graciela,Ruta,Buc...|\n",
      "|Cammy,Albares,\"Ro...|\n",
      "|Mattie,Poquette,C...|\n",
      "|Meaghan,Garufi,\"B...|\n",
      "|Gladys,Rim,T M By...|\n",
      "|Yuki,Whobrey,Farm...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"test\").getOrCreate()\n",
    "\n",
    "# Enable hadoop s3a settings\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \\\n",
    "                                     \"com.amazonaws.auth.InstanceProfileCredentialsProvider,com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3A\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Access_key_ID=\"AKIA4UOXHMLTI6TODZO6\"\n",
    "Secret_access_key=\"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\"\n",
    "\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",Access_key_ID)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",Secret_access_key)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+--------------------+-------------+--------------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
      "|first_name|last_name|        company_name|             address|         city|       country|state|  zip|age|      phone1|      phone2|               email|                 web|\n",
      "+----------+---------+--------------------+--------------------+-------------+--------------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
      "|     James|     Butt|   Benton, John B Jr|  6649 N Blue Gum St|  New Orleans|       Orleans|   LA|70116|  9|504-621-8927|504-845-1427|     jbutt@gmail.com|http://www.benton...|\n",
      "| Josephine|  Darakjy|Chanay, Jeffrey A...| 4 B Blue Ridge Blvd|     Brighton|    Livingston|   MI|48116|  8|810-292-9388|810-374-9840|josephine_darakjy...|http://www.chanay...|\n",
      "|       Art|   Venere| Chemel, James L Cpa|8 W Cerritos Ave #54|   Bridgeport|    Gloucester|   NJ| 8014|  7|856-636-8749|856-264-4130|      art@venere.org|http://www.chemel...|\n",
      "|     Lenna| Paprocki|Feltz Printing Se...|         639 Main St|    Anchorage|     Anchorage|   AK|99501| 10|907-385-4412|907-921-2010|lpaprocki@hotmail...|http://www.feltzp...|\n",
      "|   Donette|   Foller| Printing Dimensions|        34 Center St|     Hamilton|        Butler|   OH|45011| 11|513-570-1893|513-549-4561|donette.foller@co...|http://www.printi...|\n",
      "|    Simona|  Morasca| Chapman, Ross E Esq|        3 Mcauley Dr|      Ashland|       Ashland|   OH|44805| 14|419-503-2484|419-800-6759|  simona@morasca.com|http://www.chapma...|\n",
      "|    Mitsue|  Tollner|  Morlong Associates|           7 Eads St|      Chicago|          Cook|   IL|60632| 15|773-573-6914|773-924-8565|mitsue_tollner@ya...|http://www.morlon...|\n",
      "|     Leota| Dilliard|    Commercial Press|    7 W Jackson Blvd|     San Jose|   Santa Clara|   CA|95111| 20|408-752-3500|408-813-1105|   leota@hotmail.com|http://www.commer...|\n",
      "|      Sage|   Wieser|Truhlar And Truhl...|    5 Boston Ave #88|  Sioux Falls|     Minnehaha|   SD|57105| 19|605-414-2147|605-794-4895| sage_wieser@cox.net|http://www.truhla...|\n",
      "|      Kris|  Marrier|King, Christopher...|228 Runamuck Pl #...|    Baltimore|Baltimore City|   MD|21224| 25|410-655-8723|410-804-4694|      kris@gmail.com|http://www.kingch...|\n",
      "|     Minna|   Amigon|   Dorl, James J Esq|    2371 Jerrold Ave|   Kulpsville|    Montgomery|   PA|19443| 33|215-874-1229|215-422-8694|minna_amigon@yaho...|http://www.dorlja...|\n",
      "|      Abel|  Maclead| Rangoni Of Florence|  37275 St  Rt 17m M|Middle Island|       Suffolk|   NY|11953| 43|631-335-3414|631-677-3675|  amaclead@gmail.com|http://www.rangon...|\n",
      "|     Kiley|Caldarera|         Feiner Bros|    25 E 75th St #69|  Los Angeles|   Los Angeles|   CA|90034| 55|310-498-5651|310-254-3084|kiley.caldarera@a...|http://www.feiner...|\n",
      "|  Graciela|     Ruta|Buckley Miller & ...|98 Connecticut Av...|Chagrin Falls|        Geauga|   OH|44023| 33|440-780-8425|440-579-7763|       gruta@cox.net|http://www.buckle...|\n",
      "|     Cammy|  Albares|Rousseaux, Michae...|    56 E Morehead St|       Laredo|          Webb|   TX|78045| 34|956-537-6195|956-841-7216|  calbares@gmail.com|http://www.rousse...|\n",
      "|    Mattie| Poquette|Century Communica...| 73 State Road 434 E|      Phoenix|      Maricopa|   AZ|85013| 33|602-277-4385|602-953-6360|      mattie@aol.com|http://www.centur...|\n",
      "|   Meaghan|   Garufi|  Bolton, Wilbur Esq| 69734 E Carrillo St| Mc Minnville|        Warren|   TN|37110| 21|931-313-9635|931-235-7959| meaghan@hotmail.com|http://www.bolton...|\n",
      "|    Gladys|      Rim|T M Byxbee Compan...|322 New Horizon Blvd|    Milwaukee|     Milwaukee|   WI|53207| 11|414-661-9598|414-377-2880|  gladys.rim@rim.org|http://www.tmbyxb...|\n",
      "|      Yuki|  Whobrey|Farmers Insurance...|    1 State Route 27|       Taylor|         Wayne|   MI|48180| 19|313-288-7937|313-341-4470|yuki_whobrey@aol.com|http://www.farmer...|\n",
      "|  Fletcher|    Flosi|Post Box Services...| 394 Manchester Blvd|     Rockford|     Winnebago|   IL|61109|  9|815-828-2147|815-426-5657|fletcher.flosi@ya...|http://www.postbo...|\n",
      "+----------+---------+--------------------+--------------------+-------------+--------------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_uri = \"s3a://avinashhivedata/usdata1.csv\"\n",
    "data=\"s3a://s3databucket/input/us-500.csv\"\n",
    "df=spark.read.format('csv').option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(s3_uri)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2058e9c8af0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = spark.read.parquet('output/parquetdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+------+----+-----+\n",
      "|customer_id|     date|trnx_Id|amount|year|month|\n",
      "+-----------+---------+-------+------+----+-----+\n",
      "|         c1|4/12/2018|  T0051|    28|2018|   12|\n",
      "|         c1|4/12/2018|  T0091|    29|2018|   12|\n",
      "|         c2|4/12/2018|  T0091|    29|2018|   12|\n",
      "|         c1|4/12/2019|  T0023|    48|2019|   12|\n",
      "|         c1|4/12/2019|  T0043|    42|2019|   12|\n",
      "|         c2|3/12/2019|  T0513|    67|2019|   12|\n",
      "|         c2|3/12/2019|  T0014|    42|2019|   12|\n",
      "|         c2|4/12/2019|  T0043|    42|2019|   12|\n",
      "|         c1| 2/9/2019|  T0011|    20|2019|    9|\n",
      "|         c1| 2/9/2019|  T0012|    12|2019|    9|\n",
      "|         c1| 3/9/2019|  T0013|    30|2019|    9|\n",
      "|         c1| 3/9/2019|  T0014|    42|2019|    9|\n",
      "|         c2| 2/9/2019|  T0061|    25|2019|    9|\n",
      "|         c2| 2/9/2019|  T0062|    78|2019|    9|\n",
      "|         c2|2/10/2019|  T0511|    20|2019|   10|\n",
      "|         c2|2/10/2019|  T0612|    17|2019|   10|\n",
      "|         c2|4/10/2019|  T0023|    48|2019|   10|\n",
      "|         c2|4/10/2018|  T0051|    28|2018|   10|\n",
      "|         c2| 1/9/2018|  T0002|    12|2018|    9|\n",
      "|         c2| 4/9/2018|  T0003|    46|2018|    9|\n",
      "+-----------+---------+-------+------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3a://avinashhivedata/parquet_part'\n",
    "df_.write.partitionBy(\"year\",'month')\\\n",
    "    .parquet(s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3a://avinashhivedata/output'\n",
    "df_.write.format(\"csv\").mode(\"overwrite\").save(s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
