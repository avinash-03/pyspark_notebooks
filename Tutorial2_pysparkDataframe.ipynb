{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuctions\n",
    "- Pyspark Dataframe\n",
    "- Reading the dataset\n",
    "- Checking the Datatypes of teh columns(schema)\n",
    "- selecting columns and indexing\n",
    "- check Describe option similar to Pandas\n",
    "- Addding Columns\n",
    "- Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T02:50:12.659085Z",
     "start_time": "2022-02-28T02:50:12.652446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20e7c694550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Dataframe\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T03:05:55.281490Z",
     "start_time": "2022-02-28T03:05:55.098295Z"
    }
   },
   "outputs": [],
   "source": [
    "## read the dataset\n",
    "df = spark.read.option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .csv(\"dept.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T03:05:57.369893Z",
     "start_time": "2022-02-28T03:05:57.354354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departmets: string (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chek the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T02:56:00.798879Z",
     "start_time": "2022-02-28T02:56:00.776877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Departmets', 'Salary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T02:56:24.491335Z",
     "start_time": "2022-02-28T02:56:24.273692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Avi', Departmets='Data Science', Salary=10000.0),\n",
       " Row(Name='Rahul', Departmets='Big Data', Salary=15000.0),\n",
       " Row(Name='Avi', Departmets='IOT', Salary=5000.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T02:57:06.808374Z",
     "start_time": "2022-02-28T02:57:06.755239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selectiong perticular columns\n",
    "df.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T02:58:20.547824Z",
     "start_time": "2022-02-28T02:58:20.329692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|   Name| Salary|\n",
      "+-------+-------+\n",
      "|    Avi|10000.0|\n",
      "|  Rahul|15000.0|\n",
      "|    Avi| 5000.0|\n",
      "| Mahesh| 4000.0|\n",
      "|    Avi| 4000.0|\n",
      "| Mahesh| 2000.0|\n",
      "| Ganesh|10000.0|\n",
      "| Ganesh| 5000.0|\n",
      "|Shubham|10000.0|\n",
      "|Shubham| 2000.0|\n",
      "|  Ankit| 3000.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectiong mulitple columns\n",
    "df.select(['Name','Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T02:58:46.476784Z",
     "start_time": "2022-02-28T02:58:46.342954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Name'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Name'] # see the type of data # not shows the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T02:59:29.778009Z",
     "start_time": "2022-02-28T02:59:29.766029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Departmets', 'string'), ('Salary', 'double')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T03:00:08.233546Z",
     "start_time": "2022-02-28T03:00:07.716362Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----------------+\n",
      "|summary|   Name|Departmets|           Salary|\n",
      "+-------+-------+----------+-----------------+\n",
      "|  count|     11|        11|               11|\n",
      "|   mean|   null|      null|6363.636363636364|\n",
      "| stddev|   null|      null|4225.463933646276|\n",
      "|    min|  Ankit|  Big Data|           2000.0|\n",
      "|    max|Shubham|       IOT|          15000.0|\n",
      "+-------+-------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T03:06:04.646569Z",
     "start_time": "2022-02-28T03:06:04.627349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------+----------+\n",
      "|   Name|  Departmets| Salary|double_sal|\n",
      "+-------+------------+-------+----------+\n",
      "|    Avi|Data Science|10000.0|   20000.0|\n",
      "|  Rahul|    Big Data|15000.0|   30000.0|\n",
      "|    Avi|         IOT| 5000.0|   10000.0|\n",
      "| Mahesh|    Big Data| 4000.0|    8000.0|\n",
      "|    Avi|    Big Data| 4000.0|    8000.0|\n",
      "| Mahesh|Data Science| 2000.0|    4000.0|\n",
      "| Ganesh|         IOT|10000.0|   20000.0|\n",
      "| Ganesh|    Big Data| 5000.0|   10000.0|\n",
      "|Shubham|Data Science|10000.0|   20000.0|\n",
      "|Shubham|    Big Data| 2000.0|    4000.0|\n",
      "|  Ankit|    Big Data| 3000.0|    6000.0|\n",
      "+-------+------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding column \n",
    "df_pyspark=df.withColumn(\"double_sal\",df['Salary']*2)\n",
    "df_pyspark.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T03:08:13.838629Z",
     "start_time": "2022-02-28T03:08:13.708891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------+----------+\n",
      "|New Name|  Departmets| Salary|double_sal|\n",
      "+--------+------------+-------+----------+\n",
      "|     Avi|Data Science|10000.0|   20000.0|\n",
      "|   Rahul|    Big Data|15000.0|   30000.0|\n",
      "|     Avi|         IOT| 5000.0|   10000.0|\n",
      "|  Mahesh|    Big Data| 4000.0|    8000.0|\n",
      "|     Avi|    Big Data| 4000.0|    8000.0|\n",
      "|  Mahesh|Data Science| 2000.0|    4000.0|\n",
      "|  Ganesh|         IOT|10000.0|   20000.0|\n",
      "|  Ganesh|    Big Data| 5000.0|   10000.0|\n",
      "| Shubham|Data Science|10000.0|   20000.0|\n",
      "| Shubham|    Big Data| 2000.0|    4000.0|\n",
      "|   Ankit|    Big Data| 3000.0|    6000.0|\n",
      "+--------+------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rename the columns\n",
    "df_pyspark.withColumnRenamed(\"Name\",\"New Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------+\n",
      "|   Name|  Departmets| Salary|\n",
      "+-------+------------+-------+\n",
      "|    Avi|Data Science|10000.0|\n",
      "|  Rahul|    Big Data|15000.0|\n",
      "|    Avi|         IOT| 5000.0|\n",
      "| Mahesh|    Big Data| 4000.0|\n",
      "|    Avi|    Big Data| 4000.0|\n",
      "| Mahesh|Data Science| 2000.0|\n",
      "| Ganesh|         IOT|10000.0|\n",
      "| Ganesh|    Big Data| 5000.0|\n",
      "|Shubham|Data Science|10000.0|\n",
      "|Shubham|    Big Data| 2000.0|\n",
      "|  Ankit|    Big Data| 3000.0|\n",
      "+-------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# droping colunm\n",
    "df_pyspark.drop(\"double_sal\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
