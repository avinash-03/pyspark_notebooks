{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit,col,explode\n",
    "import pyspark.sql.functions as f \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL Pipeline</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ccd661da00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('data/WordData1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|This is a Japanes...|\n",
      "|The team members ...|\n",
      "|As the years pass...|\n",
      "|If you don't like...|\n",
      "|He was disappoint...|\n",
      "|When he encounter...|\n",
      "|Situps are a terr...|\n",
      "|Toddlers feeding ...|\n",
      "|Edith could decid...|\n",
      "|Her daily goal wa...|\n",
      "|Tomorrow will bri...|\n",
      "|His son quipped t...|\n",
      "|He wondered why a...|\n",
      "|If my calculator ...|\n",
      "|The hummingbird's...|\n",
      "|He went on a whis...|\n",
      "|This is the last ...|\n",
      "|I come from a tri...|\n",
      "|The delicious aro...|\n",
      "|Weather is not tr...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   words|\n",
      "+--------+\n",
      "|    This|\n",
      "|      is|\n",
      "|       a|\n",
      "|Japanese|\n",
      "|    doll|\n",
      "|     The|\n",
      "|    team|\n",
      "| members|\n",
      "|    were|\n",
      "|    hard|\n",
      "|      to|\n",
      "|    tell|\n",
      "|   apart|\n",
      "|   since|\n",
      "|    they|\n",
      "|     all|\n",
      "|    wore|\n",
      "|   their|\n",
      "|    hair|\n",
      "|      in|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transformation\n",
    "df2 = df.withColumn('splitedData',f.split('value',' '))\n",
    "df3 = df2.withColumn('words',explode('splitedData'))\n",
    "word_df = df3.select('words')\n",
    "word_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = word_df.groupBy('words').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      words|count|\n",
      "+-----------+-----+\n",
      "|   Tomorrow|    4|\n",
      "|         If|    8|\n",
      "|      leave|    4|\n",
      "|      corny|    4|\n",
      "|        day|    4|\n",
      "|preoccupied|    4|\n",
      "|       even|    8|\n",
      "|      crazy|    4|\n",
      "|    bananas|    4|\n",
      "|     priest|    4|\n",
      "|        did|    4|\n",
      "|    whether|    4|\n",
      "|     Having|    4|\n",
      "|        I'm|    4|\n",
      "|      crime|    4|\n",
      "|  surprised|    4|\n",
      "|      James|    4|\n",
      "|      could|    8|\n",
      "|        buy|    4|\n",
      "|        him|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD \n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "url = 'jdbc:mysql://database-2.cjbysn8oqrq1.us-east-1.rds.amazonaws.com/'\n",
    "table = 'avinash_schema.wordcount'\n",
    "user='admin'\n",
    "password='ad789min'\n",
    "\n",
    "word_count.write.mode('append').format(\"jdbc\").option(\"url\",url).option(\"driver\",driver)\\\n",
    "    .option(\"dbtable\",table).option(\"mode\",\"append\").option(\"user\",user)\\\n",
    "        .option(\"password\",password).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD \n",
    "driver1 = \"com.mysql.jdbc.Driver\"\n",
    "url1 = 'jdbc:mysql://localhost:3306/'\n",
    "table1 = 'study.wordcount'\n",
    "user1='root'\n",
    "password1='root'\n",
    "\n",
    "word_count.write.mode('append').format('jdbc').option(\"url\",url1)\\\n",
    "    .option(\"driver\",driver1)\\\n",
    "    .option('dbtable',table1).option('mode','append').option('user',user1)\\\n",
    "        .option('password',password1).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-227eb638c79f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#     .getOrCreate()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdataframe_mysql\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jdbc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"url\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"jdbc:mysql://localhost/exam\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession\\\n",
    "#     .builder\\\n",
    "#     .appName(\"Word Count\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "dataframe_mysql = spark.read\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost/exam\")\\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\")\\\n",
    "    .option(\"dbtable\", \"books\").option(\"user\", \"root\")\\\n",
    "    .option(\"password\", \"root\").load()\n",
    "\n",
    "print(dataframe_mysql.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----+\n",
      "|book_id|               title|price|\n",
      "+-------+--------------------+-----+\n",
      "|  12345|  Python Programming|   29|\n",
      "|  12346|         Learn MySQL|   23|\n",
      "|  12347|Data Science Cook...|   27|\n",
      "|  12367|           My dreams|   50|\n",
      "|  13456|       Eat that frog|   34|\n",
      "+-------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
