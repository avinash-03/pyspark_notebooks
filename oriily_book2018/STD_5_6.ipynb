{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark definiter gide orilly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Structured API</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27a5bd9f400>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Structured API\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Basic Structured Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"./data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mannully schme\n",
    "from tokenize import String\n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType\n",
    "\n",
    "schema = StructType(\n",
    "   [ StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"count\",LongType(),True)\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avinash Godbole\\Desktop\\Programm File\\pyspark\\oriily_book2018\\STD_5_6.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark/oriily_book2018/STD_5_6.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m flight_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark/oriily_book2018/STD_5_6.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark/oriily_book2018/STD_5_6.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m./data/flight-data/json/2015-summary.json\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(\"./data/flight-data/json/2015-summary.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row \n",
    "\n",
    "my_row = Row(\"heloo\",None,1,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heloo'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "flight_df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\"))\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty dataframe \n",
    "from pyspark.sql.types import StructField,StructType,StringType\n",
    "emt_rdd = spark.sparkContext.emptyRDD()\n",
    "col = StructType([])\n",
    "df3 = spark.createDataFrame(emt_rdd,schema=col)\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mt = spark.createDataFrame([],schema=StructType([]))\n",
    "df_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|name|year|\n",
      "+----+----+\n",
      "+----+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(year,IntegerType,true)))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_rdd = spark.sparkContext.emptyRDD()\n",
    "col = StructType([StructField(\"name\",StringType()),StructField(\"year\",StringType())])\n",
    "col2 = \"name String, year int\"\n",
    "df4 = spark.createDataFrame(emp_rdd,col2)\n",
    "df4.show()\n",
    "\n",
    "\n",
    "df4.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# in Python\n",
    "flight_df.selectExpr(\n",
    "\"*\", # all original columns\n",
    "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|one|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit \n",
    "flight_df.select(expr(\"*\"),lit(1).alias(\"one\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|number_one|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "|    United States|            Romania|   15|         1|\n",
      "|    United States|            Croatia|    1|         1|\n",
      "|    United States|            Ireland|  344|         1|\n",
      "+-----------------+-------------------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add cloumns \n",
    "flight_df.withColumn(\"number_one\",lit(1)).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|         dest|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "|United States|            Ireland|  344|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename column\n",
    "flight_df.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"dest\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: int]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the type of columns \n",
    "flight_df.withColumn(\"count2\", col(\"count\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concataning and appending rows\n",
    "schma = flight_df.schema\n",
    "new_rows = [\n",
    "    Row(\"New_country\",\"other_country\",5),\n",
    "    Row(\"new coutnry2\",\"other_country2\",1)\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_rows,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New_country|      other_country|    5|\n",
      "|     new coutnry2|     other_country2|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|     new coutnry2|     other_country2|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.union(new_df).where(\"count=1\").where(col(\"ORIGIN_COUNTRY_NAME\")!= \"United States\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partitions\n",
    "df_part=flight_df.repartition(5)\n",
    "df_part.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_df = flight_df.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collect_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = collect_df.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_df.sample(False,0.5,5).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no next item  \n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    print(next(cl))\n",
    "except StopIteration as r:\n",
    "    print(\"no next item \",r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch.6 working with different types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Booleans\n",
    "- Numbers\n",
    "- Strings\n",
    "- Dates and timestamps\n",
    "- Handling null\n",
    "- Complex types\n",
    "- User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read file\n",
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferschema\",\"true\") \\\n",
    "    .load(\"./data/retail-data/by-day/2010-12-01.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.options(inferschema=True,header=True).csv(\"./data/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types\n",
    "1. Lit \n",
    "2. Booleans -> and , or ,True, False \n",
    "3. instr\n",
    "4. Numbers\n",
    "5. row_number -> monotonically_increasing_id \n",
    "6. capital,lower,upper -> initcap,lower,upper \n",
    "7. strip -> ltrim,rtrim,trim,lpad,rpad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lit \n",
    "from pyspark.sql.functions import  lit \n",
    "df.select(lit(5),lit(\"five\"),lit(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "|536365   |SET 7 BABUSHKA NESTING BOXES       |\n",
      "|536365   |GLASS STAR FROSTED T-LIGHT HOLDER  |\n",
      "+---------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# boolean, where\n",
    "from pyspark.sql.functions import col \n",
    "df.where(col(\"InvoiceNo\")==536365) \\\n",
    "    .select(\"InvoiceNo\",\"Description\") \\\n",
    "    .show(8,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instr\n",
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") >600\n",
    "decriFilter = instr(df.Description,\"POSTAGE\") >=1\n",
    "\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | decriFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|unitPrice|is_expensive|\n",
      "+---------+------------+\n",
      "|   569.77|        true|\n",
      "|   607.49|        true|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"is_expensive\",(col(\"StockCode\")==\"DOT\") & (priceFilter | decriFilter)) \\\n",
    "    .where(\"is_expensive\").select(\"unitPrice\",\"is_expensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# null\n",
    "df.where(col(\"Description\").eqNullSafe(\"hello\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "|   17850.0|             489.0|\n",
      "+----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# numbers\n",
    "from pyspark.sql.functions import  expr, pow\n",
    "\n",
    "fabricatedQuantity = pow(col(\"Quantity\")*col(\"UnitPrice\"),2) + 5 \n",
    "df.select(expr(\"CustomerID\"),fabricatedQuantity.alias(\"realQuantity\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|round(2.76656, 0)|bround(1.4677, 0)|\n",
      "+-----------------+-----------------+\n",
      "|              3.0|              1.0|\n",
      "|              3.0|              1.0|\n",
      "|              3.0|              1.0|\n",
      "+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# round \n",
    "from pyspark.sql.functions import lit,round,bround\n",
    "\n",
    "df.select(round(lit(2.76656)),bround(lit(\"1.4677\"))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# corr -> Pearson correlation coefficient\n",
    "from pyspark.sql.functions import  corr\n",
    "df.stat.corr(\"Quantity\",\"UnitPrice\")\n",
    "df.select(corr(\"Quantity\",\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|        InvoiceDate|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|               3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128|               null| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|               null|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|2010-12-01 08:26:00|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|2010-12-01 17:35:00|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decribie static parameter\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab(\"StockCode\",\"Quantity\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------+\n",
      "|(monotonically_increasing_id() + 1)|invoiceNo|\n",
      "+-----------------------------------+---------+\n",
      "|                                  1|   536365|\n",
      "|                                  2|   536365|\n",
      "|                                  3|   536365|\n",
      "+-----------------------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # row num\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "df.select(monotonically_increasing_id()+1,\"invoiceNo\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "|Cream Cupid Heart...|\n",
      "|Knitted Union Fla...|\n",
      "|Red Woolly Hottie...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|cream cupid heart...|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|knitted union fla...|KNITTED UNION FLA...|\n",
      "|RED WOOLLY HOTTIE...|red woolly hottie...|RED WOOLLY HOTTIE...|\n",
      "|SET 7 BABUSHKA NE...|set 7 babushka ne...|SET 7 BABUSHKA NE...|\n",
      "|GLASS STAR FROSTE...|glass star froste...|GLASS STAR FROSTE...|\n",
      "|HAND WARMER UNION...|hand warmer union...|HAND WARMER UNION...|\n",
      "|HAND WARMER RED P...|hand warmer red p...|HAND WARMER RED P...|\n",
      "|ASSORTED COLOUR B...|assorted colour b...|ASSORTED COLOUR B...|\n",
      "|POPPY'S PLAYHOUSE...|poppy's playhouse...|POPPY'S PLAYHOUSE...|\n",
      "|POPPY'S PLAYHOUSE...|poppy's playhouse...|POPPY'S PLAYHOUSE...|\n",
      "|FELTCRAFT PRINCES...|feltcraft princes...|FELTCRAFT PRINCES...|\n",
      "|IVORY KNITTED MUG...|ivory knitted mug...|IVORY KNITTED MUG...|\n",
      "|BOX OF 6 ASSORTED...|box of 6 assorted...|BOX OF 6 ASSORTED...|\n",
      "|BOX OF VINTAGE JI...|box of vintage ji...|BOX OF VINTAGE JI...|\n",
      "|BOX OF VINTAGE AL...|box of vintage al...|BOX OF VINTAGE AL...|\n",
      "|HOME BUILDING BLO...|home building blo...|HOME BUILDING BLO...|\n",
      "|LOVE BUILDING BLO...|love building blo...|LOVE BUILDING BLO...|\n",
      "|RECIPE BOX WITH M...|recipe box with m...|RECIPE BOX WITH M...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower,upper ,initcap \n",
    "\n",
    "df.select(col(\"Description\"),lower(col(\"Description\")),upper(col(\"Description\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       hello|\n",
      "+------------+\n",
      "|#######hello|\n",
      "|#######hello|\n",
      "|#######hello|\n",
      "|#######hello|\n",
      "|#######hello|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import  lit,rtrim,ltrim,trim,lpad,rpad\n",
    "\n",
    "str1= \"      hello       \"\n",
    "str2 = \"hello\"\n",
    "df.select(lpad(lit(str2),12,\"#\").alias(\"hello\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. regexp_replace(col,old_str,new_str)\n",
    "2. regexp_extract(col,ext_str,1)\n",
    "3. translate(col,\"LEET\",\"1337\") -> PLET - P137\n",
    "4. contains \n",
    "5. loacate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-----------------------------------+\n",
      "|color_clean                        |Description                        |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "|COLOR HANGING HEART T-LIGHT HOLDER |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|COLOR METAL LANTERN                |WHITE METAL LANTERN                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|COLOR WOOLLY HOTTIE COLOR HEART.   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "|SET 7 BABUSHKA NESTING BOXES       |SET 7 BABUSHKA NESTING BOXES       |\n",
      "+-----------------------------------+-----------------------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "reg_str = \"BLACK|WHITE|RED|GREEEN|BLUE\"\n",
    "\n",
    "df.select(regexp_replace(col(\"Description\"),reg_str,\"COLOR\").alias(\"color_clean\"),col(\"Description\")).show(6,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1338)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI83 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI83 M38A1 1AN83RN| WHITE METAL LANTERN|\n",
      "|              CR3AM CUPID H3AR8...|CREAM CUPID HEART...|\n",
      "|              KNI883D UNION F1A...|KNITTED UNION FLA...|\n",
      "|              R3D WOO11Y HO88I3...|RED WOOLLY HOTTIE...|\n",
      "|              S38 7 BABUSHKA N3...|SET 7 BABUSHKA NE...|\n",
      "|              G1ASS S8AR FROS83...|GLASS STAR FROSTE...|\n",
      "|              HAND WARM3R UNION...|HAND WARMER UNION...|\n",
      "|              HAND WARM3R R3D P...|HAND WARMER RED P...|\n",
      "|              ASSOR83D CO1OUR B...|ASSORTED COLOUR B...|\n",
      "|              POPPY'S P1AYHOUS3...|POPPY'S PLAYHOUSE...|\n",
      "|              POPPY'S P1AYHOUS3...|POPPY'S PLAYHOUSE...|\n",
      "|              F318CRAF8 PRINC3S...|FELTCRAFT PRINCES...|\n",
      "|              IVORY KNI883D MUG...|IVORY KNITTED MUG...|\n",
      "|              BOX OF 6 ASSOR83D...|BOX OF 6 ASSORTED...|\n",
      "|              BOX OF VIN8AG3 JI...|BOX OF VINTAGE JI...|\n",
      "|              BOX OF VIN8AG3 A1...|BOX OF VINTAGE AL...|\n",
      "|              HOM3 BUI1DING B1O...|HOME BUILDING BLO...|\n",
      "|              1OV3 BUI1DING B1O...|LOVE BUILDING BLO...|\n",
      "|              R3CIP3 BOX WI8H M...|RECIPE BOX WITH M...|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.select(translate(col(\"Description\"),\"LEET\",\"1338\"),col(\"Description\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------------+\n",
      "|color_clean|Description                        |\n",
      "+-----------+-----------------------------------+\n",
      "|WHITE      |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|WHITE      |WHITE METAL LANTERN                |\n",
      "|           |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|           |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|RED        |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+-----------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "ext_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "    regexp_extract(col(\"Description\"),ext_str,1).alias(\"color_clean\"),\n",
    "    col(\"Description\")\n",
    ").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|WOOD 2 DRAWER CABINET WHITE FINISH|\n",
      "|WOOD S/3 CABINET ANT WHITE FINISH |\n",
      "|WOODEN PICTURE FRAME WHITE FINISH |\n",
      "|WOODEN FRAME ANTIQUE WHITE        |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr \n",
    "is_black = instr(col(\"Description\"),\"BLACK\") >=1\n",
    "is_white = instr(col(\"Description\"),\"WHITE\") >=1\n",
    "\n",
    "df.withColumn(\"has_simple_color\", is_black | is_white ) \\\n",
    "    .where(\"has_simple_color\") \\\n",
    "    .select(\"Description\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|RED WOOLLY HOTTIE...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate \n",
    "simple_color = [\"black\",\"white\",\"red\",\"green\", \"blue\"]\n",
    "\n",
    "def color_locator(col,color_str1):\n",
    "    return locate(color_str1.upper(),col).cast(\"boolean\").alias(\"is_\"+ color_str1)\n",
    "\n",
    "selected_col = [color_locator(df.Description,c) for c in simple_color]\n",
    "selected_col.append(expr(\"*\"))\n",
    "\n",
    "df.select(*selected_col) \\\n",
    "    .where(expr(\"is_white OR is_red\")).\\\n",
    "    select(\"Description\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and timestaps \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "\n",
    "date_df = spark.range(10) \\\n",
    "    .withColumn(\"today\",current_date()) \\\n",
    "    .withColumn(\"now\",current_timestamp())\n",
    "\n",
    "date_df.createOrReplaceTempView(\"datetb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|1  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|2  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|3  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|4  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|5  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|6  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|7  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|8  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "|9  |2022-10-12|2022-10-12 12:09:02.835|\n",
      "+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2022-10-12|2022-10-12 12:09:...|\n",
      "|  1|2022-10-12|2022-10-12 12:09:...|\n",
      "|  2|2022-10-12|2022-10-12 12:09:...|\n",
      "|  3|2022-10-12|2022-10-12 12:09:...|\n",
      "|  4|2022-10-12|2022-10-12 12:09:...|\n",
      "|  5|2022-10-12|2022-10-12 12:09:...|\n",
      "|  6|2022-10-12|2022-10-12 12:09:...|\n",
      "|  7|2022-10-12|2022-10-12 12:09:...|\n",
      "|  8|2022-10-12|2022-10-12 12:09:...|\n",
      "|  9|2022-10-12|2022-10-12 12:09:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from datetb\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "|        2022-10-07|        2022-10-17|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "date_df.select(date_sub(col(\"today\"),5),date_add(col(\"today\"),5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------+----------+--------+\n",
      "|id |today     |now                   |week_ago  |day_diff|\n",
      "+---+----------+----------------------+----------+--------+\n",
      "|0  |2022-10-12|2022-10-12 12:14:06.46|2022-10-05|-7      |\n",
      "|1  |2022-10-12|2022-10-12 12:14:06.46|2022-10-05|-7      |\n",
      "|2  |2022-10-12|2022-10-12 12:14:06.46|2022-10-05|-7      |\n",
      "|3  |2022-10-12|2022-10-12 12:14:06.46|2022-10-05|-7      |\n",
      "|4  |2022-10-12|2022-10-12 12:14:06.46|2022-10-05|-7      |\n",
      "+---+----------+----------------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# diffreence betewwn two dates\n",
    "from pyspark.sql.functions import datediff,months_between, to_date \n",
    "date_df.withColumn(\"week_ago\",date_sub(col(\"today\"),7))\\\n",
    "    .withColumn(\"day_diff\",datediff(col(\"week_ago\"),col(\"today\"))).show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\")\n",
    ").select(months_between(col(\"start\"),col(\"end\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|to_date('13-12-2016')|\n",
      "+---------------------+\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "|                 null|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.select(to_date(lit(\"13-12-2016\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "to_date(lit(\"2017-12-13\"), \"yyyy-MM-dd\").alias(\"date\"),\n",
    "to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n",
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-01-13|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "|             CREAM CUPID HEART...|\n",
      "|             KNITTED UNION FLA...|\n",
      "|             RED WOOLLY HOTTIE...|\n",
      "|             SET 7 BABUSHKA NE...|\n",
      "|             GLASS STAR FROSTE...|\n",
      "|             HAND WARMER UNION...|\n",
      "|             HAND WARMER RED P...|\n",
      "|             ASSORTED COLOUR B...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             POPPY'S PLAYHOUSE...|\n",
      "|             FELTCRAFT PRINCES...|\n",
      "|             IVORY KNITTED MUG...|\n",
      "|             BOX OF 6 ASSORTED...|\n",
      "|             BOX OF VINTAGE JI...|\n",
      "|             BOX OF VINTAGE AL...|\n",
      "|             HOME BUILDING BLO...|\n",
      "|             LOVE BUILDING BLO...|\n",
      "|             RECIPE BOX WITH M...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # coalesce returns the frist not null values from columns\n",
    "from pyspark.sql.functions import coalesce \n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|ifnull(NULL, 'return_value')|nullif('value', 'value')|nvl(NULL, 'return_value')|nvl2('not_null', 'return_value', 'else_value')|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "|                return_value|                    null|             return_value|                                  return_value|\n",
      "+----------------------------+------------------------+-------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ifnull,nullif, nvl, nvl2\n",
    "spark.sql(\n",
    "    \"\"\"select ifnull(null,'return_value'), nullif('value','value'),\n",
    "        nvl(null, 'return_value'),\n",
    "        nvl2('not_null', 'return_value', \"else_value\")\n",
    "\"\"\"\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#d drop\n",
    "df.na.drop()\n",
    "df.na.drop(\"any\")\n",
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill\n",
    "df.na.fill(\"vaule_string\")\n",
    "df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])\n",
    "\n",
    "\n",
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  struct\n",
    "\n",
    "comlex_df = df.select(struct(\"Description\",\"InvoiceNo\").alias(\"complex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- complex: struct (nullable = false)\n",
      " |    |-- Description: string (nullable = true)\n",
      " |    |-- InvoiceNo: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comlex_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "|     [CREAM, CUPID, HE...|\n",
      "+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(col(\"Description\"),\" \")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "|       CREAM|\n",
      "|     KNITTED|\n",
      "|         RED|\n",
      "|         SET|\n",
      "|       GLASS|\n",
      "|        HAND|\n",
      "|        HAND|\n",
      "|    ASSORTED|\n",
      "|     POPPY'S|\n",
      "|     POPPY'S|\n",
      "|   FELTCRAFT|\n",
      "|       IVORY|\n",
      "|         BOX|\n",
      "|         BOX|\n",
      "|         BOX|\n",
      "|        HOME|\n",
      "|        LOVE|\n",
      "|      RECIPE|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"),\" \").alias(\"array_col\")).select(col(\"array_col\")[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "|                              5|\n",
      "+-------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "from pyspark.sql.functions import size \n",
    "df.select(size(split(col(\"Description\"),\" \"))).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "|                                           false|\n",
      "+------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array contains\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"),\" \"),\"WHITE\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "|WHITE HANGING HEA...|   536365|   HEART|\n",
      "|WHITE HANGING HEA...|   536365| T-LIGHT|\n",
      "|WHITE HANGING HEA...|   536365|  HOLDER|\n",
      "| WHITE METAL LANTERN|   536365|   WHITE|\n",
      "| WHITE METAL LANTERN|   536365|   METAL|\n",
      "| WHITE METAL LANTERN|   536365| LANTERN|\n",
      "|CREAM CUPID HEART...|   536365|   CREAM|\n",
      "|CREAM CUPID HEART...|   536365|   CUPID|\n",
      "|CREAM CUPID HEART...|   536365|  HEARTS|\n",
      "|CREAM CUPID HEART...|   536365|    COAT|\n",
      "|CREAM CUPID HEART...|   536365|  HANGER|\n",
      "|KNITTED UNION FLA...|   536365| KNITTED|\n",
      "|KNITTED UNION FLA...|   536365|   UNION|\n",
      "|KNITTED UNION FLA...|   536365|    FLAG|\n",
      "|KNITTED UNION FLA...|   536365|     HOT|\n",
      "|KNITTED UNION FLA...|   536365|   WATER|\n",
      "|KNITTED UNION FLA...|   536365|  BOTTLE|\n",
      "|RED WOOLLY HOTTIE...|   536365|     RED|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode\n",
    "from pyspark.sql.functions import explode\n",
    "df.withColumn(\"splitted\",split(col(\"Description\"),\" \")) \\\n",
    "    .withColumn(\"exploded\",explode(col(\"splitted\"))) \\\n",
    "        .select(\"Description\",\"InvoiceNo\",\"exploded\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|complex_map                                   |\n",
      "+----------------------------------------------+\n",
      "|[WHITE HANGING HEART T-LIGHT HOLDER -> 536365]|\n",
      "|[WHITE METAL LANTERN -> 536365]               |\n",
      "+----------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# maps \n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "complex_map = df.select(create_map(col(\"Description\"),col(\"InvoiceNo\")).alias(\"complex_map\"))\n",
    "complex_map.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "|                            null|\n",
      "|                            null|\n",
      "|                            null|\n",
      "+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_map.selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "|CREAM CUPID HEART...|536365|\n",
      "|KNITTED UNION FLA...|536365|\n",
      "|RED WOOLLY HOTTIE...|536365|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_map.select(explode(col('complex_map'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                 key| value|\n",
      "+--------------------+------+\n",
      "|WHITE HANGING HEA...|536365|\n",
      "| WHITE METAL LANTERN|536365|\n",
      "+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_map.select(explode('complex_map')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = spark.range(1).selectExpr(\n",
    "    \"\"\" '{\"my_json_key\":{\n",
    "        \"my_json_value\":[1,2,3] }}' as json_string \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         json_string|\n",
      "+--------------------+\n",
      "|{\"my_json_key\":{\n",
      "...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+--------------------+\n",
      "|get_json_object(json_string, $.my_json_key.my_json_value[1])|                  c0|\n",
      "+------------------------------------------------------------+--------------------+\n",
      "|                                                           2|{\"my_json_value\":...|\n",
      "+------------------------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "json_df.select(\n",
    "    get_json_object(col(\"json_string\"), \"$.my_json_key.my_json_value[1]\"),\n",
    "    json_tuple(col(\"json_string\"),\"my_json_key\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|to_json(my_struct)                                                       |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}    |\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as my_struct\")\\\n",
    "    .select(to_json(col(\"my_struct\"))).show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+--------------------------------------------------------------------------+\n",
      "|from_json(new_json)                          |new_json                                                                  |\n",
      "+---------------------------------------------+--------------------------------------------------------------------------+\n",
      "|[536365, WHITE HANGING HEART T-LIGHT HOLDER] |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"} |\n",
      "|[536365, WHITE METAL LANTERN]                |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}                |\n",
      "|[536365, CREAM CUPID HEARTS COAT HANGER]     |{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}     |\n",
      "|[536365, KNITTED UNION FLAG HOT WATER BOTTLE]|{\"InvoiceNo\":\"536365\",\"Description\":\"KNITTED UNION FLAG HOT WATER BOTTLE\"}|\n",
      "+---------------------------------------------+--------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StringType,StructField,StructType\n",
    "\n",
    "parse_schema = StructType(\n",
    "    (\n",
    "        StructField(\"InvoiceNo\",StringType(),True),\n",
    "        StructField(\"Description\",StringType(),True)\n",
    "    )\n",
    ")\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as my_struct\") \\\n",
    "    .select(to_json(col(\"my_struct\")).alias(\"new_json\"))\\\n",
    "        .select(from_json(col(\"new_json\"),parse_schema),col(\"new_json\")).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "\n",
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          8|\n",
      "|         27|\n",
      "|         64|\n",
      "|        125|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# power 3 \n",
    "from pyspark.sql.functions import udf\n",
    "udf_df = spark.range(1,6).toDF(\"num\")\n",
    "udf_df.show()\n",
    "\n",
    "def power3(val):\n",
    "    return val**3\n",
    "\n",
    "pow3_udf = udf(power3)\n",
    "\n",
    "udf_df.select(pow3_udf(col(\"num\"))).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.power3(val)>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"power3r\",power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|power3r(num)|\n",
      "+------------+\n",
      "|           1|\n",
      "|           8|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_df.selectExpr(\"power3r(num)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def power3(number:Double):Double = number * number * number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################ end ##############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
