{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"7_8\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>7_8</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1daad59d4c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .load(\"./data/retail-data/all/*.csv\")\\\n",
    "    .coalesce(5)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView(\"dftable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='581585', StockCode='22466', Description='FAIRY TALE COTTAGE NIGHT LIGHT', Quantity=12, InvoiceDate='12/9/2011 12:31', UnitPrice=1.95, CustomerID=15804, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='22061', Description='LARGE CAKE STAND  HANGING STRAWBERY', Quantity=8, InvoiceDate='12/9/2011 12:49', UnitPrice=2.95, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='23275', Description='SET OF 3 HANGING OWLS OLLIE BEAK', Quantity=24, InvoiceDate='12/9/2011 12:49', UnitPrice=1.25, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='21217', Description='RED RETROSPOT ROUND CAKE TINS', Quantity=24, InvoiceDate='12/9/2011 12:49', UnitPrice=8.95, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='20685', Description='DOORMAT RED RETROSPOT', Quantity=10, InvoiceDate='12/9/2011 12:49', UnitPrice=7.08, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581587', StockCode='22631', Description='CIRCUS PARADE LUNCH BOX ', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.95, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22556', Description='PLASTERS IN TIN CIRCUS PARADE ', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.65, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22555', Description='PLASTERS IN TIN STRONGMAN', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.65, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22728', Description='ALARM CLOCK BAKELIKE PINK', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22727', Description='ALARM CLOCK BAKELIKE RED ', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22726', Description='ALARM CLOCK BAKELIKE GREEN', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22730', Description='ALARM CLOCK BAKELIKE IVORY', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22367', Description='CHILDRENS APRON SPACEBOY DESIGN', Quantity=8, InvoiceDate='12/9/2011 12:50', UnitPrice=1.95, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22629', Description='SPACEBOY LUNCH BOX ', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.95, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='23256', Description='CHILDRENS CUTLERY SPACEBOY ', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=4.15, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22613', Description='PACK OF 20 SPACEBOY NAPKINS', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=0.85, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22899', Description=\"CHILDREN'S APRON DOLLY GIRL \", Quantity=6, InvoiceDate='12/9/2011 12:50', UnitPrice=2.1, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='23254', Description='CHILDRENS CUTLERY DOLLY GIRL ', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=4.15, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='23255', Description='CHILDRENS CUTLERY CIRCUS PARADE', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=4.15, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22138', Description='BAKING SET 9 PIECE RETROSPOT ', Quantity=3, InvoiceDate='12/9/2011 12:50', UnitPrice=4.95, CustomerID=12680, Country='France')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch7. AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "from pyspark.sql.functions import count ,countDistinct\n",
    "\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count ,countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\",0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first,last\n",
    "\n",
    "df.select(first(\"StockCode\"),last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max \n",
    "df.select(min(\"Quantity\"),max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum \n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in python\n",
    "from pyspark.sql.functions import var_pop, stddev_pop \n",
    "from pyspark.sql.functions import var_samp,stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"),var_samp(\"Quantity\"),stddev_pop(\"Quantity\"),stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052562|119768.05495536952|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import kurtosis,skewness\n",
    "df.select(skewness(\"Quantity\"),kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"),collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   536596|      null|    6|\n",
      "|   537252|      null|    1|\n",
      "|   538041|      null|    1|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\",\"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "from pyspark.sql.functions import col, to_date,to_timestamp\n",
    "\n",
    "df_date = df.withColumn(\"date\",to_date(col(\"InvoiceDate\"),\"MM/d/yyyy H:mm\"))\n",
    "df_date.createOrReplaceTempView(\"dfdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "|2010-12-01|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.select(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc \n",
    "\n",
    "windo_spec = Window\\\n",
    "    .partitionBy(\"CustomerId\",\"date\") \\\n",
    "    .orderBy(desc(\"Quantity\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "from pyspark.sql.functions import max, dense_rank, rank  \n",
    "max_purchaseQuantity = max(col(\"Quantity\")).over(windo_spec)\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windo_spec)\n",
    "purchaseRank = rank().over(windo_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-------------+-------------------+----------------+\n",
      "|CustomerId|      date|Quantity|quantity_rank|quantity_dense_rank|max_purchaseQuan|\n",
      "+----------+----------+--------+-------------+-------------------+----------------+\n",
      "|     12346|2011-01-18|   74215|            1|                  1|           74215|\n",
      "|     12346|2011-01-18|  -74215|            2|                  2|           74215|\n",
      "|     12347|2010-12-07|      36|            1|                  1|              36|\n",
      "|     12347|2010-12-07|      30|            2|                  2|              36|\n",
      "|     12347|2010-12-07|      24|            3|                  3|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|       6|           17|                  5|              36|\n",
      "|     12347|2010-12-07|       6|           17|                  5|              36|\n",
      "+----------+----------+--------+-------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df_date.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "    .select(\n",
    "        col(\"CustomerId\"),\n",
    "        col(\"date\"),\n",
    "        col(\"Quantity\"),\n",
    "        purchaseRank.alias(\"quantity_rank\"),\n",
    "        purchaseDenseRank.alias(\"quantity_dense_rank\"),\n",
    "        max_purchaseQuantity.alias(\"max_purchaseQuan\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.select(max_purchaseQuantity).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_df = df_date.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406829"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_null_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null = df_date.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum \n",
    "roll_up_df = no_null.rollup(\"date\",\"Country\").agg(sum(\"Quantity\"))\\\n",
    "    .select(\"date\",\"Country\",(\"sum(Quantity)\").alias(\"total_quan\")).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+\n",
      "|      date|       Country|total_quan|\n",
      "+----------+--------------+----------+\n",
      "|      null|          null|   5176450|\n",
      "|2010-12-01|   Netherlands|        97|\n",
      "|2010-12-01|United Kingdom|     23949|\n",
      "|2010-12-01|        Norway|      1852|\n",
      "|2010-12-01|        France|       449|\n",
      "|2010-12-01|          EIRE|       243|\n",
      "|2010-12-01|       Germany|       117|\n",
      "|2010-12-01|     Australia|       107|\n",
      "|2010-12-01|          null|     26814|\n",
      "|2010-12-02|          EIRE|         4|\n",
      "|2010-12-02|          null|     21023|\n",
      "|2010-12-02|       Germany|       146|\n",
      "|2010-12-02|United Kingdom|     20873|\n",
      "|2010-12-03|       Belgium|       528|\n",
      "|2010-12-03|        France|       239|\n",
      "|2010-12-03|       Germany|       170|\n",
      "|2010-12-03|         Spain|       400|\n",
      "|2010-12-03|         Italy|       164|\n",
      "|2010-12-03|   Switzerland|       110|\n",
      "|2010-12-03|          null|     14830|\n",
      "+----------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roll_up_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+\n",
      "|      date|Country|total_quan|\n",
      "+----------+-------+----------+\n",
      "|      null|   null|   5176450|\n",
      "|2010-12-01|   null|     26814|\n",
      "|2010-12-02|   null|     21023|\n",
      "|2010-12-03|   null|     14830|\n",
      "|2010-12-05|   null|     16395|\n",
      "|2010-12-06|   null|     21419|\n",
      "|2010-12-07|   null|     24995|\n",
      "|2010-12-08|   null|     22741|\n",
      "|2010-12-09|   null|     18431|\n",
      "|2010-12-10|   null|     20297|\n",
      "|2010-12-12|   null|     10565|\n",
      "|2010-12-13|   null|     17623|\n",
      "|2010-12-14|   null|     20098|\n",
      "|2010-12-15|   null|     18229|\n",
      "|2010-12-16|   null|     29632|\n",
      "|2010-12-17|   null|     16069|\n",
      "|2010-12-19|   null|      3795|\n",
      "|2010-12-20|   null|     14965|\n",
      "|2010-12-21|   null|     15467|\n",
      "|2010-12-22|   null|      3192|\n",
      "+----------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the grand total where null is \n",
    "roll_up_df.where(\"Country is Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "|date|Country|total_quan|\n",
      "+----+-------+----------+\n",
      "|null|   null|   5176450|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where \n",
    "roll_up_df.where(\"Date is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|                null|      5176450|\n",
      "|null|               Japan|        25218|\n",
      "|null|         Unspecified|         3300|\n",
      "|null|           Australia|        83653|\n",
      "|null|            Portugal|        16180|\n",
      "|null|             Finland|        10666|\n",
      "|null|                 RSA|          352|\n",
      "|null|             Germany|       117448|\n",
      "|null|             Lebanon|          386|\n",
      "|null|              Cyprus|         6317|\n",
      "|null|                 USA|         1034|\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|           Hong Kong|         4769|\n",
      "|null|           Singapore|         5234|\n",
      "|null|              Norway|        19247|\n",
      "|null|               Spain|        26824|\n",
      "|null|     Channel Islands|         9479|\n",
      "|null|  European Community|          497|\n",
      "|null|      Czech Republic|          592|\n",
      "|null|             Denmark|         8188|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_null.cube(\"date\",\"Country\").agg(sum(col(\"Quantity\")))\\\n",
    "    .select(\"date\",\"Country\",\"sum(Quantity)\").orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-------------+\n",
      "|customerId|StockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|     13767|    21484|            0|            8|\n",
      "|     15862|    22384|            0|            1|\n",
      "|     16218|    22383|            0|          150|\n",
      "|     14729|    22919|            0|            2|\n",
      "|     15525|    22411|            0|            2|\n",
      "|     15485|    22819|            0|           36|\n",
      "|     12433|    21981|            0|           96|\n",
      "|     13093|    22960|            0|           48|\n",
      "|     16274|    21147|            0|            2|\n",
      "|     13576|    21756|            0|            7|\n",
      "|     18011|    22910|            0|            1|\n",
      "|     15658|    21756|            0|            3|\n",
      "|     14901|    22301|            0|            6|\n",
      "|     13117|    84879|            0|            8|\n",
      "|     15574|    22659|            0|            1|\n",
      "|     15574|    21587|            0|            6|\n",
      "|     14775|    22405|            0|           12|\n",
      "|     15299|    22275|            0|           69|\n",
      "|     18239|    21349|            0|            2|\n",
      "|     17552|    21033|            0|           10|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grouping id \n",
    "from pyspark.sql.functions import grouping_id,sum, expr \n",
    "no_null.cube(\"customerId\",\"StockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    "    .orderBy(grouping_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df_date.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|      date|USA_sum(CAST(Quantity AS BIGINT))|\n",
      "+----------+---------------------------------+\n",
      "|2011-12-06|                             null|\n",
      "|2011-12-09|                             null|\n",
      "|2011-12-08|                             -196|\n",
      "|2011-12-07|                             null|\n",
      "+----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\" ,\"`USA_sum(CAST(Quantity AS BIGINT))`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'Australia_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Australia_sum(UnitPrice)',\n",
       " 'Australia_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Austria_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Austria_sum(UnitPrice)',\n",
       " 'Austria_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Bahrain_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Bahrain_sum(UnitPrice)',\n",
       " 'Bahrain_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Belgium_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Belgium_sum(UnitPrice)',\n",
       " 'Belgium_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Brazil_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Brazil_sum(UnitPrice)',\n",
       " 'Brazil_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Canada_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Canada_sum(UnitPrice)',\n",
       " 'Canada_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Channel Islands_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Channel Islands_sum(UnitPrice)',\n",
       " 'Channel Islands_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Cyprus_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Cyprus_sum(UnitPrice)',\n",
       " 'Cyprus_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Czech Republic_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Czech Republic_sum(UnitPrice)',\n",
       " 'Czech Republic_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Denmark_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Denmark_sum(UnitPrice)',\n",
       " 'Denmark_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'EIRE_sum(CAST(Quantity AS BIGINT))',\n",
       " 'EIRE_sum(UnitPrice)',\n",
       " 'EIRE_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'European Community_sum(CAST(Quantity AS BIGINT))',\n",
       " 'European Community_sum(UnitPrice)',\n",
       " 'European Community_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Finland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Finland_sum(UnitPrice)',\n",
       " 'Finland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'France_sum(CAST(Quantity AS BIGINT))',\n",
       " 'France_sum(UnitPrice)',\n",
       " 'France_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Germany_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Germany_sum(UnitPrice)',\n",
       " 'Germany_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Greece_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Greece_sum(UnitPrice)',\n",
       " 'Greece_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Hong Kong_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Hong Kong_sum(UnitPrice)',\n",
       " 'Hong Kong_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Iceland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Iceland_sum(UnitPrice)',\n",
       " 'Iceland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Israel_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Israel_sum(UnitPrice)',\n",
       " 'Israel_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Italy_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Italy_sum(UnitPrice)',\n",
       " 'Italy_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Japan_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Japan_sum(UnitPrice)',\n",
       " 'Japan_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Lebanon_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Lebanon_sum(UnitPrice)',\n",
       " 'Lebanon_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Lithuania_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Lithuania_sum(UnitPrice)',\n",
       " 'Lithuania_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Malta_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Malta_sum(UnitPrice)',\n",
       " 'Malta_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Netherlands_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Netherlands_sum(UnitPrice)',\n",
       " 'Netherlands_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Norway_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Norway_sum(UnitPrice)',\n",
       " 'Norway_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Poland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Poland_sum(UnitPrice)',\n",
       " 'Poland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Portugal_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Portugal_sum(UnitPrice)',\n",
       " 'Portugal_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'RSA_sum(CAST(Quantity AS BIGINT))',\n",
       " 'RSA_sum(UnitPrice)',\n",
       " 'RSA_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Saudi Arabia_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Saudi Arabia_sum(UnitPrice)',\n",
       " 'Saudi Arabia_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Singapore_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Singapore_sum(UnitPrice)',\n",
       " 'Singapore_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Spain_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Spain_sum(UnitPrice)',\n",
       " 'Spain_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Sweden_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Sweden_sum(UnitPrice)',\n",
       " 'Sweden_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Switzerland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Switzerland_sum(UnitPrice)',\n",
       " 'Switzerland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'USA_sum(CAST(Quantity AS BIGINT))',\n",
       " 'USA_sum(UnitPrice)',\n",
       " 'USA_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'United Arab Emirates_sum(CAST(Quantity AS BIGINT))',\n",
       " 'United Arab Emirates_sum(UnitPrice)',\n",
       " 'United Arab Emirates_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'United Kingdom_sum(CAST(Quantity AS BIGINT))',\n",
       " 'United Kingdom_sum(UnitPrice)',\n",
       " 'United Kingdom_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Unspecified_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Unspecified_sum(UnitPrice)',\n",
       " 'Unspecified_sum(CAST(CustomerID AS BIGINT))']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Country|\n",
      "+--------------------+\n",
      "|         Unspecified|\n",
      "|      United Kingdom|\n",
      "|United Arab Emirates|\n",
      "|                 USA|\n",
      "|         Switzerland|\n",
      "|              Sweden|\n",
      "|               Spain|\n",
      "|           Singapore|\n",
      "|        Saudi Arabia|\n",
      "|                 RSA|\n",
      "|            Portugal|\n",
      "|              Poland|\n",
      "|              Norway|\n",
      "|         Netherlands|\n",
      "|               Malta|\n",
      "|           Lithuania|\n",
      "|             Lebanon|\n",
      "|               Japan|\n",
      "|               Italy|\n",
      "|              Israel|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\").distinct().orderBy(col(\"Country\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Inner Joins \n",
    "- 2. Outer Joins \n",
    "- 3. Left Outer joins \n",
    "- 4. Right Outer joins\n",
    "- 5. Left semi joins  \n",
    "- 6. Left anti joins\n",
    "- 7. Natural Joins \n",
    "- 8. Cross joins    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  0|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  3|             avi|               4|          [100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# person data\n",
    "person =  spark.createDataFrame(\n",
    "    [\n",
    "        (0,\"Bill Chambers\",0,[100]),\n",
    "        (1,\"Matei Zaharia\",1,[500,250,100]),\n",
    "        (2,\"Michael Armbrust\",1,[250,100]),(3,\"avi\",4,[100])\n",
    "    ],schema=[\"id\",\"name\",\"graduate_program\",\"spark_status\"])\n",
    "person.show()\n",
    "\n",
    "\n",
    "\n",
    "# graduated df \n",
    "grad_data = [\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")]\n",
    "col = [\"id\", \"degree\", \"department\", \"school\"]\n",
    "\n",
    "graduated_df = spark.createDataFrame(grad_data,schema=col)\n",
    "\n",
    "# spark status df \n",
    "spark_data = [(500, \"Vice President\"),\n",
    "(250, \"PMC Member\"),\n",
    "(100, \"Contributor\")]\n",
    "spark_schema =[\"id\",'status']\n",
    "\n",
    "spark_status_df = spark.createDataFrame(spark_data,schema=spark_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduated_df.createOrReplaceTempView(\"graduateProgram\")\n",
    "spark_status_df.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  0|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "join_expr = person[\"graduate_program\"]==graduated_df['id']\n",
    "\n",
    "person.join(graduated_df,person.graduate_program==graduated_df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   0|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# outer join\n",
    "person.join(graduated_df,person.graduate_program==graduated_df.id,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|  id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|   0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|   1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  0|Michael Armbrust|               1|     [250, 100]|   1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|null|   null|                null|       null|\n",
      "+---+----------------+----------------+---------------+----+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left outer join \n",
    "person.join(graduated_df,join_expr,\"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   0|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# right_outer jon\n",
    "person.join(graduated_df,join_expr,\"right_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left_semi\n",
    "graduated_df.join(person,graduated_df.id==person.graduate_program,'left_semi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left antijoin\n",
    "graduated_df.join(person,graduated_df.id==person.graduate_program,'left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   Bill Chambers|               0|          [100]|\n",
      "|  0|Masters|School of Informa...|UC Berkeley|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM graduateProgram NATURAL JOIN person').show(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Unsupported using join type Cross",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-e83d2661cdb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mjoin_expr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"graduate_program\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mgraduated_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mjoinType\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"cross\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgraduated_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoinType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how)\u001b[0m\n\u001b[0;32m   1146\u001b[0m                 \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"how should be basestring\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Unsupported using join type Cross"
     ]
    }
   ],
   "source": [
    "join_expr = person[\"graduate_program\"]==graduated_df['id']\n",
    "joinType = \"cross\"\n",
    "graduated_df.join(person,how=joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduated_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "|       3|             avi|               4|          [100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "from pyspark.sql.functions import expr \n",
    "\n",
    "person.withColumnRenamed(\"id\",\"personId\")\\\n",
    "    .join(spark_status_df,expr(\"array_contains(spark_status,id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graduate_program', 'degree', 'department', 'school']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graduate_dum1 = graduated_df.withColumnRenamed(\"id\",\"graduate_program\")\n",
    "\n",
    "graduate_dum1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduated_df,person.graduate_program==graduated_df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduate_dum1,person.graduate_program==graduate_dum1.graduate_program).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduate_dum1,\"graduate_program\").select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
