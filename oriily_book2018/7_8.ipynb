{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter_7_8</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e942e11a00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession.builder.appName(\"chapter_7_8\").getOrCreate()\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferschema\",\"true\")\\\n",
    "    .load(\"./data/retail-data/all/*.csv\")\\\n",
    "    .coalesce(5)\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,StringType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,IntegerType,true),StructField(Country,StringType,true)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"./data/retail-data/all/*.csv\")\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView(\"dftable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(InvoiceNo='581585', StockCode='22466', Description='FAIRY TALE COTTAGE NIGHT LIGHT', Quantity=12, InvoiceDate='12/9/2011 12:31', UnitPrice=1.95, CustomerID=15804, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='22061', Description='LARGE CAKE STAND  HANGING STRAWBERY', Quantity=8, InvoiceDate='12/9/2011 12:49', UnitPrice=2.95, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='23275', Description='SET OF 3 HANGING OWLS OLLIE BEAK', Quantity=24, InvoiceDate='12/9/2011 12:49', UnitPrice=1.25, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='21217', Description='RED RETROSPOT ROUND CAKE TINS', Quantity=24, InvoiceDate='12/9/2011 12:49', UnitPrice=8.95, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581586', StockCode='20685', Description='DOORMAT RED RETROSPOT', Quantity=10, InvoiceDate='12/9/2011 12:49', UnitPrice=7.08, CustomerID=13113, Country='United Kingdom'),\n",
       " Row(InvoiceNo='581587', StockCode='22631', Description='CIRCUS PARADE LUNCH BOX ', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.95, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22556', Description='PLASTERS IN TIN CIRCUS PARADE ', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.65, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22555', Description='PLASTERS IN TIN STRONGMAN', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.65, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22728', Description='ALARM CLOCK BAKELIKE PINK', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22727', Description='ALARM CLOCK BAKELIKE RED ', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22726', Description='ALARM CLOCK BAKELIKE GREEN', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22730', Description='ALARM CLOCK BAKELIKE IVORY', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=3.75, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22367', Description='CHILDRENS APRON SPACEBOY DESIGN', Quantity=8, InvoiceDate='12/9/2011 12:50', UnitPrice=1.95, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22629', Description='SPACEBOY LUNCH BOX ', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=1.95, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='23256', Description='CHILDRENS CUTLERY SPACEBOY ', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=4.15, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22613', Description='PACK OF 20 SPACEBOY NAPKINS', Quantity=12, InvoiceDate='12/9/2011 12:50', UnitPrice=0.85, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22899', Description=\"CHILDREN'S APRON DOLLY GIRL \", Quantity=6, InvoiceDate='12/9/2011 12:50', UnitPrice=2.1, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='23254', Description='CHILDRENS CUTLERY DOLLY GIRL ', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=4.15, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='23255', Description='CHILDRENS CUTLERY CIRCUS PARADE', Quantity=4, InvoiceDate='12/9/2011 12:50', UnitPrice=4.15, CustomerID=12680, Country='France'),\n",
       " Row(InvoiceNo='581587', StockCode='22138', Description='BAKING SET 9 PIECE RETROSPOT ', Quantity=3, InvoiceDate='12/9/2011 12:50', UnitPrice=4.95, CustomerID=12680, Country='France')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch7. AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "from pyspark.sql.functions import count ,countDistinct\n",
    "\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count ,countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\",0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first,last\n",
    "\n",
    "df.select(first(\"StockCode\"),last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max \n",
    "df.select(min(\"Quantity\"),max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum \n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609165|47559.391409298856|   218.0809566344782|    218.0811578502344|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in python\n",
    "from pyspark.sql.functions import var_pop, stddev_pop \n",
    "from pyspark.sql.functions import var_samp,stddev_samp\n",
    "\n",
    "df.select(var_pop(\"Quantity\"),var_samp(\"Quantity\"),stddev_pop(\"Quantity\"),stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052369|119768.05495536828|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import kurtosis,skewness\n",
    "df.select(skewness(\"Quantity\"),kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085630787E-4|             1052.7280543892425|            1052.7260778731384|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"),collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   536596|      null|    6|\n",
      "|   537252|      null|    1|\n",
      "|   538041|      null|    1|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\",\"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "from pyspark.sql.functions import col, to_date,to_timestamp\n",
    "\n",
    "df_date = df.withColumn(\"date\",to_date(col(\"InvoiceDate\"),\"MM/d/yyyy H:mm\"))\n",
    "df_date.createOrReplaceTempView(\"dfdate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|      date|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|2010-12-01|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|2010-12-01|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|2010-12-01|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|2010-12-01|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|2010-12-01|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|2010-12-01|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dfdate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc \n",
    "\n",
    "windo_spec = Window\\\n",
    "    .partitionBy(\"CustomerId\",\"date\") \\\n",
    "    .orderBy(desc(\"Quantity\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "\n",
    "from pyspark.sql.functions import max, dense_rank, rank  \n",
    "max_purchaseQuantity = max(col(\"Quantity\")).over(windo_spec)\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windo_spec)\n",
    "purchaseRank = rank().over(windo_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-------------+-------------------+----------------+\n",
      "|CustomerId|      date|Quantity|quantity_rank|quantity_dense_rank|max_purchaseQuan|\n",
      "+----------+----------+--------+-------------+-------------------+----------------+\n",
      "|     12346|2011-01-18|   74215|            1|                  1|           74215|\n",
      "|     12346|2011-01-18|  -74215|            2|                  2|           74215|\n",
      "|     12347|2010-12-07|      36|            1|                  1|              36|\n",
      "|     12347|2010-12-07|      30|            2|                  2|              36|\n",
      "|     12347|2010-12-07|      24|            3|                  3|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|      12|            4|                  4|              36|\n",
      "|     12347|2010-12-07|       6|           17|                  5|              36|\n",
      "|     12347|2010-12-07|       6|           17|                  5|              36|\n",
      "+----------+----------+--------+-------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col \n",
    "\n",
    "df_date.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "    .select(\n",
    "        col(\"CustomerId\"),\n",
    "        col(\"date\"),\n",
    "        col(\"Quantity\"),\n",
    "        purchaseRank.alias(\"quantity_rank\"),\n",
    "        purchaseDenseRank.alias(\"quantity_dense_rank\"),\n",
    "        max_purchaseQuantity.alias(\"max_purchaseQuan\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "|                                                                                                                                  100|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.select(max_purchaseQuantity).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null_df = df_date.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406829"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_null_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_null = df_date.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum \n",
    "roll_up_df = no_null.rollup(\"date\",\"Country\").agg(sum(\"Quantity\").alias(\"total_quan\"))\\\n",
    "    .select(\"date\",\"Country\",\"total_quan\").orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+\n",
      "|      date|       Country|total_quan|\n",
      "+----------+--------------+----------+\n",
      "|      null|          null|   5176450|\n",
      "|2010-12-01|United Kingdom|     23949|\n",
      "|2010-12-01|          EIRE|       243|\n",
      "|2010-12-01|     Australia|       107|\n",
      "|2010-12-01|       Germany|       117|\n",
      "|2010-12-01|        France|       449|\n",
      "|2010-12-01|          null|     26814|\n",
      "|2010-12-01|        Norway|      1852|\n",
      "|2010-12-01|   Netherlands|        97|\n",
      "|2010-12-02|       Germany|       146|\n",
      "|2010-12-02|          null|     21023|\n",
      "|2010-12-02|United Kingdom|     20873|\n",
      "|2010-12-02|          EIRE|         4|\n",
      "|2010-12-03|         Italy|       164|\n",
      "|2010-12-03|        Poland|       140|\n",
      "|2010-12-03|       Germany|       170|\n",
      "|2010-12-03|         Spain|       400|\n",
      "|2010-12-03|        France|       239|\n",
      "|2010-12-03|          null|     14830|\n",
      "|2010-12-03|   Switzerland|       110|\n",
      "+----------+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roll_up_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+\n",
      "|      date|Country|total_quan|\n",
      "+----------+-------+----------+\n",
      "|      null|   null|   5176450|\n",
      "|2010-12-01|   null|     26814|\n",
      "|2010-12-02|   null|     21023|\n",
      "|2010-12-03|   null|     14830|\n",
      "|2010-12-05|   null|     16395|\n",
      "|2010-12-06|   null|     21419|\n",
      "|2010-12-07|   null|     24995|\n",
      "|2010-12-08|   null|     22741|\n",
      "|2010-12-09|   null|     18431|\n",
      "|2010-12-10|   null|     20297|\n",
      "|2010-12-12|   null|     10565|\n",
      "|2010-12-13|   null|     17623|\n",
      "|2010-12-14|   null|     20098|\n",
      "|2010-12-15|   null|     18229|\n",
      "|2010-12-16|   null|     29632|\n",
      "|2010-12-17|   null|     16069|\n",
      "|2010-12-19|   null|      3795|\n",
      "|2010-12-20|   null|     14965|\n",
      "|2010-12-21|   null|     15467|\n",
      "|2010-12-22|   null|      3192|\n",
      "+----------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the grand total where null is \n",
    "roll_up_df.where(\"Country is Null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "|date|Country|total_quan|\n",
      "+----+-------+----------+\n",
      "|null|   null|   5176450|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where \n",
    "roll_up_df.where(\"Date is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|                null|      5176450|\n",
      "|null|               Japan|        25218|\n",
      "|null|         Unspecified|         3300|\n",
      "|null|           Australia|        83653|\n",
      "|null|            Portugal|        16180|\n",
      "|null|             Finland|        10666|\n",
      "|null|                 RSA|          352|\n",
      "|null|             Germany|       117448|\n",
      "|null|             Lebanon|          386|\n",
      "|null|              Cyprus|         6317|\n",
      "|null|                 USA|         1034|\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|           Hong Kong|         4769|\n",
      "|null|           Singapore|         5234|\n",
      "|null|              Norway|        19247|\n",
      "|null|               Spain|        26824|\n",
      "|null|     Channel Islands|         9479|\n",
      "|null|  European Community|          497|\n",
      "|null|      Czech Republic|          592|\n",
      "|null|             Denmark|         8188|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_null.cube(\"date\",\"Country\").agg(sum(col(\"Quantity\")))\\\n",
    "    .select(\"date\",\"Country\",\"sum(Quantity)\").orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-------------+\n",
      "|customerId|StockCode|grouping_id()|sum(Quantity)|\n",
      "+----------+---------+-------------+-------------+\n",
      "|     13767|    21484|            0|            8|\n",
      "|     15862|    22384|            0|            1|\n",
      "|     16218|    22383|            0|          150|\n",
      "|     14729|    22919|            0|            2|\n",
      "|     15525|    22411|            0|            2|\n",
      "|     15485|    22819|            0|           36|\n",
      "|     12433|    21981|            0|           96|\n",
      "|     13093|    22960|            0|           48|\n",
      "|     16274|    21147|            0|            2|\n",
      "|     13576|    21756|            0|            7|\n",
      "|     18011|    22910|            0|            1|\n",
      "|     15658|    21756|            0|            3|\n",
      "|     14901|    22301|            0|            6|\n",
      "|     13117|    84879|            0|            8|\n",
      "|     15574|    22659|            0|            1|\n",
      "|     15574|    21587|            0|            6|\n",
      "|     14775|    22405|            0|           12|\n",
      "|     15299|    22275|            0|           69|\n",
      "|     18239|    21349|            0|            2|\n",
      "|     17552|    21033|            0|           10|\n",
      "+----------+---------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# grouping id \n",
    "from pyspark.sql.functions import grouping_id,sum, expr \n",
    "no_null.cube(\"customerId\",\"StockCode\").agg(grouping_id(), sum(\"Quantity\"))\\\n",
    "    .orderBy(grouping_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df_date.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+\n",
      "|      date|USA_sum(CAST(Quantity AS BIGINT))|\n",
      "+----------+---------------------------------+\n",
      "|2011-12-06|                             null|\n",
      "|2011-12-09|                             null|\n",
      "|2011-12-08|                             -196|\n",
      "|2011-12-07|                             null|\n",
      "+----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\" ,\"`USA_sum(CAST(Quantity AS BIGINT))`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'Australia_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Australia_sum(UnitPrice)',\n",
       " 'Australia_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Austria_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Austria_sum(UnitPrice)',\n",
       " 'Austria_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Bahrain_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Bahrain_sum(UnitPrice)',\n",
       " 'Bahrain_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Belgium_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Belgium_sum(UnitPrice)',\n",
       " 'Belgium_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Brazil_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Brazil_sum(UnitPrice)',\n",
       " 'Brazil_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Canada_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Canada_sum(UnitPrice)',\n",
       " 'Canada_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Channel Islands_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Channel Islands_sum(UnitPrice)',\n",
       " 'Channel Islands_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Cyprus_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Cyprus_sum(UnitPrice)',\n",
       " 'Cyprus_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Czech Republic_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Czech Republic_sum(UnitPrice)',\n",
       " 'Czech Republic_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Denmark_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Denmark_sum(UnitPrice)',\n",
       " 'Denmark_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'EIRE_sum(CAST(Quantity AS BIGINT))',\n",
       " 'EIRE_sum(UnitPrice)',\n",
       " 'EIRE_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'European Community_sum(CAST(Quantity AS BIGINT))',\n",
       " 'European Community_sum(UnitPrice)',\n",
       " 'European Community_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Finland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Finland_sum(UnitPrice)',\n",
       " 'Finland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'France_sum(CAST(Quantity AS BIGINT))',\n",
       " 'France_sum(UnitPrice)',\n",
       " 'France_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Germany_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Germany_sum(UnitPrice)',\n",
       " 'Germany_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Greece_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Greece_sum(UnitPrice)',\n",
       " 'Greece_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Hong Kong_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Hong Kong_sum(UnitPrice)',\n",
       " 'Hong Kong_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Iceland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Iceland_sum(UnitPrice)',\n",
       " 'Iceland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Israel_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Israel_sum(UnitPrice)',\n",
       " 'Israel_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Italy_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Italy_sum(UnitPrice)',\n",
       " 'Italy_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Japan_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Japan_sum(UnitPrice)',\n",
       " 'Japan_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Lebanon_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Lebanon_sum(UnitPrice)',\n",
       " 'Lebanon_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Lithuania_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Lithuania_sum(UnitPrice)',\n",
       " 'Lithuania_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Malta_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Malta_sum(UnitPrice)',\n",
       " 'Malta_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Netherlands_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Netherlands_sum(UnitPrice)',\n",
       " 'Netherlands_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Norway_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Norway_sum(UnitPrice)',\n",
       " 'Norway_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Poland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Poland_sum(UnitPrice)',\n",
       " 'Poland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Portugal_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Portugal_sum(UnitPrice)',\n",
       " 'Portugal_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'RSA_sum(CAST(Quantity AS BIGINT))',\n",
       " 'RSA_sum(UnitPrice)',\n",
       " 'RSA_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Saudi Arabia_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Saudi Arabia_sum(UnitPrice)',\n",
       " 'Saudi Arabia_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Singapore_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Singapore_sum(UnitPrice)',\n",
       " 'Singapore_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Spain_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Spain_sum(UnitPrice)',\n",
       " 'Spain_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Sweden_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Sweden_sum(UnitPrice)',\n",
       " 'Sweden_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Switzerland_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Switzerland_sum(UnitPrice)',\n",
       " 'Switzerland_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'USA_sum(CAST(Quantity AS BIGINT))',\n",
       " 'USA_sum(UnitPrice)',\n",
       " 'USA_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'United Arab Emirates_sum(CAST(Quantity AS BIGINT))',\n",
       " 'United Arab Emirates_sum(UnitPrice)',\n",
       " 'United Arab Emirates_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'United Kingdom_sum(CAST(Quantity AS BIGINT))',\n",
       " 'United Kingdom_sum(UnitPrice)',\n",
       " 'United Kingdom_sum(CAST(CustomerID AS BIGINT))',\n",
       " 'Unspecified_sum(CAST(Quantity AS BIGINT))',\n",
       " 'Unspecified_sum(UnitPrice)',\n",
       " 'Unspecified_sum(CAST(CustomerID AS BIGINT))']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             Country|\n",
      "+--------------------+\n",
      "|         Unspecified|\n",
      "|      United Kingdom|\n",
      "|United Arab Emirates|\n",
      "|                 USA|\n",
      "|         Switzerland|\n",
      "|              Sweden|\n",
      "|               Spain|\n",
      "|           Singapore|\n",
      "|        Saudi Arabia|\n",
      "|                 RSA|\n",
      "|            Portugal|\n",
      "|              Poland|\n",
      "|              Norway|\n",
      "|         Netherlands|\n",
      "|               Malta|\n",
      "|           Lithuania|\n",
      "|             Lebanon|\n",
      "|               Japan|\n",
      "|               Italy|\n",
      "|              Israel|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Country\").distinct().orderBy(col(\"Country\").desc()).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Joins"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. Inner Joins \n",
    "- 2. Outer Joins \n",
    "- 3. Left Outer joins \n",
    "- 4. Right Outer joins\n",
    "- 5. Left semi joins  \n",
    "- 6. Left anti joins\n",
    "- 7. Natural Joins \n",
    "- 8. Cross joins    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  0|Michael Armbrust|               1|     [250, 100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+\n",
      "| id|            name|graduate_program|   spark_status|\n",
      "+---+----------------+----------------+---------------+\n",
      "|  0|   Bill Chambers|               0|          [100]|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|\n",
      "|  3|             avi|               4|          [100]|\n",
      "+---+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# person data\n",
    "person =  spark.createDataFrame(\n",
    "    [\n",
    "        (0,\"Bill Chambers\",0,[100]),\n",
    "        (1,\"Matei Zaharia\",1,[500,250,100]),\n",
    "        (2,\"Michael Armbrust\",1,[250,100]),(3,\"avi\",4,[100])\n",
    "    ],schema=[\"id\",\"name\",\"graduate_program\",\"spark_status\"])\n",
    "person.show()\n",
    "\n",
    "\n",
    "\n",
    "# graduated df \n",
    "grad_data = [\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")]\n",
    "col = [\"id\", \"degree\", \"department\", \"school\"]\n",
    "\n",
    "graduated_df = spark.createDataFrame(grad_data,schema=col)\n",
    "\n",
    "# spark status df \n",
    "spark_data = [(500, \"Vice President\"),\n",
    "(250, \"PMC Member\"),\n",
    "(100, \"Contributor\")]\n",
    "spark_schema =[\"id\",'status']\n",
    "\n",
    "spark_status_df = spark.createDataFrame(spark_data,schema=spark_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduated_df.createOrReplaceTempView(\"graduateProgram\")\n",
    "spark_status_df.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  0|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "join_expr = person[\"graduate_program\"]==graduated_df['id']\n",
    "\n",
    "person.join(graduated_df,person.graduate_program==graduated_df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   0|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# outer join\n",
    "person.join(graduated_df,person.graduate_program==graduated_df.id,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|  id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|   0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|   1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  0|Michael Armbrust|               1|     [250, 100]|   1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|null|   null|                null|       null|\n",
      "+---+----------------+----------------+---------------+----+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left outer join \n",
    "person.join(graduated_df,join_expr,\"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   0|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# right_outer jon\n",
    "person.join(graduated_df,join_expr,\"right_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left_semi\n",
    "graduated_df.join(person,graduated_df.id==person.graduate_program,'left_semi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left antijoin\n",
    "graduated_df.join(person,graduated_df.id==person.graduate_program,'left_anti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "| id| degree|          department|     school|            name|graduate_program|   spark_status|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|   Bill Chambers|               0|          [100]|\n",
      "|  0|Masters|School of Informa...|UC Berkeley|Michael Armbrust|               1|     [250, 100]|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|   Matei Zaharia|               1|[500, 250, 100]|\n",
      "+---+-------+--------------------+-----------+----------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM graduateProgram NATURAL JOIN person').show(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Unsupported using join type Cross",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-e83d2661cdb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mjoin_expr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperson\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"graduate_program\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mgraduated_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mjoinType\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"cross\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgraduated_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoinType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how)\u001b[0m\n\u001b[0;32m   1146\u001b[0m                 \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"how should be basestring\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Unsupported using join type Cross"
     ]
    }
   ],
   "source": [
    "join_expr = person[\"graduate_program\"]==graduated_df['id']\n",
    "joinType = \"cross\"\n",
    "graduated_df.join(person,how=joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  3|             avi|               4|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.crossJoin(graduated_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "|       3|             avi|               4|          [100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "from pyspark.sql.functions import expr \n",
    "\n",
    "person.withColumnRenamed(\"id\",\"personId\")\\\n",
    "    .join(spark_status_df,expr(\"array_contains(spark_status,id)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graduate_program', 'degree', 'department', 'school']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graduate_dum1 = graduated_df.withColumnRenamed(\"id\",\"graduate_program\")\n",
    "\n",
    "graduate_dum1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduated_df,person.graduate_program==graduated_df.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduate_dum1,person.graduate_program==graduate_dum1.graduate_program).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|graduate_program|\n",
      "+----------------+\n",
      "|               0|\n",
      "|               1|\n",
      "|               1|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person.join(graduate_dum1,\"graduate_program\").select(\"graduate_program\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arr = [([[\"a\",\"b\",\"c\",\"c\"],[\"e\",\"f\",\"g\"],['h','i','g']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'c']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avinash Godbole\\Desktop\\Programm File\\pyspark\\oriily_book2018\\7_8.ipynb Cell 60\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark/oriily_book2018/7_8.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df4 \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(arr[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m],schema\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcol1 string,col2 stirng,col3 string ,col4 string\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\session.py:605\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    602\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(\n\u001b[0;32m    604\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 605\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\session.py:630\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    628\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    629\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 630\u001b[0m     rdd, schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[0;32m    631\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n\u001b[0;32m    632\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession\u001b[39m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[39m.\u001b[39mrdd(), schema\u001b[39m.\u001b[39mjson())\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\session.py:451\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    448\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[0;32m    450\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 451\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    452\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    453\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\session.py:383\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(first) \u001b[39mis\u001b[39;00m \u001b[39mdict\u001b[39m:\n\u001b[0;32m    381\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39minferring schema from dict is deprecated,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mplease use pyspark.sql.Row instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 383\u001b[0m schema \u001b[39m=\u001b[39m reduce(_merge_type, (_infer_schema(row, names) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data))\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\session.py:383\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(first) \u001b[39mis\u001b[39;00m \u001b[39mdict\u001b[39m:\n\u001b[0;32m    381\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39minferring schema from dict is deprecated,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mplease use pyspark.sql.Row instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 383\u001b[0m schema \u001b[39m=\u001b[39m reduce(_merge_type, (_infer_schema(row, names) \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m data))\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\types.py:1067\u001b[0m, in \u001b[0;36m_infer_schema\u001b[1;34m(row, names)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     items \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(row\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mitems())\n\u001b[0;32m   1066\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not infer schema for type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(row))\n\u001b[0;32m   1069\u001b[0m fields \u001b[39m=\u001b[39m [StructField(k, _infer_type(v), \u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m items]\n\u001b[0;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "df4 = spark.createDataFrame(arr[0][0],schema=[\"col1 string,col2 stirng,col3 string ,col4 string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o60.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 5.0 failed 1 times, most recent failure: Lost task 2.0 in stage 5.0 (TID 15, avinash, executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 3 values are provided.\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:742)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 3 values are provided.\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:742)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Avinash Godbole\\Desktop\\Programm File\\pyspark\\oriily_book2018\\7_8.ipynb Cell 61\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Avinash%20Godbole/Desktop/Programm%20File/pyspark/oriily_book2018/7_8.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df4\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\dataframe.py:440\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[0;32m    408\u001b[0m \u001b[39m:param n: Number of rows to show.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39m name | Bob\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 440\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mshowString(n, \u001b[39mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\pyspark\\sql\\utils.py:128\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    130\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark3\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o60.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 5.0 failed 1 times, most recent failure: Lost task 2.0 in stage 5.0 (TID 15, avinash, executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 3 values are provided.\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:742)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 4 fields are required while 3 values are provided.\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\r\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:742)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
