{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://avinash:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rdd</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x12f67680580>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"rdd\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "|     7|\n",
      "|     8|\n",
      "|     9|\n",
      "|    10|\n",
      "|    11|\n",
      "|    12|\n",
      "|    13|\n",
      "|    14|\n",
      "|    15|\n",
      "|    16|\n",
      "|    17|\n",
      "|    18|\n",
      "|    19|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_range = spark.range(100).toDF(\"number\")\n",
    "my_range.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     2|\n",
      "|     4|\n",
      "|     6|\n",
      "|     8|\n",
      "|    10|\n",
      "|    12|\n",
      "|    14|\n",
      "|    16|\n",
      "|    18|\n",
      "|    20|\n",
      "|    22|\n",
      "|    24|\n",
      "|    26|\n",
      "|    28|\n",
      "|    30|\n",
      "|    32|\n",
      "|    34|\n",
      "|    36|\n",
      "|    38|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_range.where(\"number %2 ==0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divis_by_2 = my_range.where(\"number%2 = 0\").take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# end to end example \n",
    "flight_data_2015 = spark.read\\\n",
    "                    .option(\"inferSchema\",True)\\\n",
    "                    .option(\"header\",True)\\\n",
    "                    .csv(\"data/flight-data/csv/2015-summary.csv\")\n",
    "\n",
    "flight_data_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_data_2015.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_2015.createOrReplaceTempView(\"flight_data_15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+------+\n",
      "| DEST_COUNTRY_NAME| average| total|\n",
      "+------------------+--------+------+\n",
      "|     United States|3290.816|411352|\n",
      "|            Canada|  8399.0|  8399|\n",
      "|            Mexico|  7140.0|  7140|\n",
      "|    United Kingdom|  2025.0|  2025|\n",
      "|             Japan|  1548.0|  1548|\n",
      "|           Germany|  1468.0|  1468|\n",
      "|Dominican Republic|  1353.0|  1353|\n",
      "|       South Korea|  1048.0|  1048|\n",
      "|       The Bahamas|   955.0|   955|\n",
      "|            France|   935.0|   935|\n",
      "|          Colombia|   873.0|   873|\n",
      "|            Brazil|   853.0|   853|\n",
      "|       Netherlands|   776.0|   776|\n",
      "|             China|   772.0|   772|\n",
      "|           Jamaica|   666.0|   666|\n",
      "|        Costa Rica|   588.0|   588|\n",
      "|       El Salvador|   561.0|   561|\n",
      "|            Panama|   510.0|   510|\n",
      "|              Cuba|   466.0|   466|\n",
      "|             Spain|   420.0|   420|\n",
      "+------------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------------+--------+------+\n",
      "| DEST_COUNTRY_NAME| average| total|\n",
      "+------------------+--------+------+\n",
      "|     United States|3290.816|411352|\n",
      "|            Canada|  8399.0|  8399|\n",
      "|            Mexico|  7140.0|  7140|\n",
      "|    United Kingdom|  2025.0|  2025|\n",
      "|             Japan|  1548.0|  1548|\n",
      "|           Germany|  1468.0|  1468|\n",
      "|Dominican Republic|  1353.0|  1353|\n",
      "|       South Korea|  1048.0|  1048|\n",
      "|       The Bahamas|   955.0|   955|\n",
      "|            France|   935.0|   935|\n",
      "|          Colombia|   873.0|   873|\n",
      "|            Brazil|   853.0|   853|\n",
      "|       Netherlands|   776.0|   776|\n",
      "|             China|   772.0|   772|\n",
      "|           Jamaica|   666.0|   666|\n",
      "|        Costa Rica|   588.0|   588|\n",
      "|       El Salvador|   561.0|   561|\n",
      "|            Panama|   510.0|   510|\n",
      "|              Cuba|   466.0|   466|\n",
      "|             Spain|   420.0|   420|\n",
      "+------------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find toatal flights per country\n",
    "from pyspark.sql import functions as F \n",
    "flight_data_2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\").agg(F.avg(\"count\").alias(\"average\"),F.sum(\"count\").alias(\"total\"))\\\n",
    "    .orderBy(F.col(\"total\").desc()).show()\n",
    "\n",
    "# using sql\n",
    "spark.sql(\"select DEST_COUNTRY_NAME, avg(count) as average , sum(count) as total from flight_data_15 group by DEST_COUNTRY_NAME order by total desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F \n",
    "\n",
    "flight_data_2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "    .sort(F.desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"rdd\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lower level api \n",
    "- rdd \n",
    "- accumulatorw\n",
    "- broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rdd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\n",
    "\n",
    "words = spark.sparkContext.parallelize(collection,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark',\n",
       " 'The',\n",
       " 'Definitive',\n",
       " 'Guide',\n",
       " ':',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Processing',\n",
       " 'Made',\n",
       " 'Simple']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mywords ParallelCollectionRDD[115] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.setName(\"mywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RDD.name of mywords ParallelCollectionRDD[115] at readRDDFromFile at PythonRDD.scala:262>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spark', 'The']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transoformation\n",
    "1. distinct \n",
    "2. map \n",
    "3. filter \n",
    "4. flatmap \n",
    "5. sort \n",
    "6. random split \n",
    "\n",
    "## action \n",
    "1. reduce \n",
    "2. count / countApprox\n",
    "3. countByValue\n",
    "4. first\n",
    "5. max and min \n",
    "6. take\n",
    "7.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving files \n",
    "1. saveAsTextFile\n",
    "2. sequence file saveAsObjectFile\n",
    "3. \n",
    "\n",
    "## caching\n",
    "wrods.cache()\n",
    "\n",
    "# checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  partition\n",
    "1. mapPartition\n",
    "2. foreachPartition\n",
    "3. glom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Spark', 'The'],\n",
       " ['Definitive', 'Guide'],\n",
       " [':', 'Big'],\n",
       " ['Data', 'Processing', 'Made', 'Simple']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "words = spark.sparkContext.parallelize(collection,4)\n",
    "words.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
